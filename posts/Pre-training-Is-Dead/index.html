

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/YXY.png">
  <link rel="icon" href="/img/YXY.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#c30000">
  <meta name="author" content="Xiyuan Yang">
  <meta name="keywords" content="Code">
  
    <meta name="description" content="This blog focuses on the speech delivered by Ilya Sutskever in Neurips 2024, which discussed the future of scaling law and pre-training method, as well as the future directions of AI development.">
<meta property="og:type" content="article">
<meta property="og:title" content="Pre-Training-Is-Dead?">
<meta property="og:url" content="https://xiyuanyang-code.github.io/posts/Pre-training-Is-Dead/index.html">
<meta property="og:site_name" content="Xiyuan Yang&#39;s Blog">
<meta property="og:description" content="This blog focuses on the speech delivered by Ilya Sutskever in Neurips 2024, which discussed the future of scaling law and pre-training method, as well as the future directions of AI development.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xiyuanyang-code.github.io/img/cover/pretraining.png">
<meta property="article:published_time" content="2025-03-08T16:36:50.000Z">
<meta property="article:modified_time" content="2025-05-12T09:07:09.389Z">
<meta property="article:author" content="Xiyuan Yang">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Finished">
<meta property="article:tag" content="Pretraining">
<meta property="article:tag" content="Large Language Models">
<meta property="article:tag" content="Celebrity speeches">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://xiyuanyang-code.github.io/img/cover/pretraining.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Pre-Training-Is-Dead? - Xiyuan Yang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xiyuanyang-code.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4","placement":"left","visible":"hover","icon":"❡"},"progressbar":{"enable":true,"height_px":3,"color":"red","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"L7r0uGb0fafbzNvmBADCMH42-gzGzoHsz","app_key":"2Lr1fQ2rjhwRiUrDx0VOQyUm","server_url":null,"path":"window.location.pathname","ignore_local":true},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Xiyuan Yang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://xiyuanyang-code.github.io/posts/Above-All-en/" target="_self">
                <i class="iconfont icon-bookmark-fill"></i>
                <span>Intro</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archive</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Category</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tag</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://xiyuanyang-code.github.io/resume/" target="_self">
                <i class="iconfont icon-code"></i>
                <span>Resume</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>Else</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/FAQ/" target="_self">
                    <i class="iconfont icon-bug"></i>
                    <span>FAQ</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/links" target="_self">
                    <i class="iconfont icon-link-fill"></i>
                    <span>Links</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/Blog-word-counting/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>Status</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/posts/My-Posts/" target="_self">
                    <i class="iconfont icon-notebook"></i>
                    <span>All Posts</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/Recordings/" target="_self">
                    <i class="iconfont icon-clipcheck"></i>
                    <span>Daily Loggings</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://github.com/xiyuanyang-code" target="_self">
                    <i class="iconfont icon-github-fill"></i>
                    <span>My Github</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/" target="_self">
                    <i class="iconfont icon-copyright"></i>
                    <span>About Hexo</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/posts/Life-musings/" target="_self">
                    <i class="iconfont icon-brush"></i>
                    <span>Life Musing</span>
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/place.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Pre-Training-Is-Dead?"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Xiyuan Yang
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-09 00:36" pubdate>
          March 9, 2025 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          537 words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Artificial Intelligence"
        id="heading-5cd2adc9e2a5254e4c1da803519f298b" role="tab" data-toggle="collapse" href="#collapse-5cd2adc9e2a5254e4c1da803519f298b"
        aria-expanded="true"
      >
        Artificial Intelligence
        <span class="list-group-count">(17)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-5cd2adc9e2a5254e4c1da803519f298b"
           role="tabpanel" aria-labelledby="heading-5cd2adc9e2a5254e4c1da803519f298b">
        
        
          
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/Pre-training-Is-Dead/" title="Pre-Training-Is-Dead?"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">Pre-Training-Is-Dead?</span>
        </a>
      
    
      
      
        <a href="/posts/Deep-Learning-Memo/" title="Deep_Learning_Memo"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Deep_Learning_Memo</span>
        </a>
      
    
      
      
        <a href="/posts/RL-speeches/" title="RL_speeches"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">RL_speeches</span>
        </a>
      
    
      
      
        <a href="/posts/AI-Paper-2024/" title="AI-Paper-2024"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AI-Paper-2024</span>
        </a>
      
    
      
      
        <a href="/posts/RAG-tutorial/" title="RAG-Tutorial"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">RAG-Tutorial</span>
        </a>
      
    
      
      
        <a href="/posts/Imagenet/" title="ImageNet and ILSVRC"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">ImageNet and ILSVRC</span>
        </a>
      
    
      
      
        <a href="/posts/LLM-Evaluating/" title="LLM Evaluating"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">LLM Evaluating</span>
        </a>
      
    
      
      
        <a href="/posts/Factor-Mining-in-Quantitative-Investing-A-Survey/" title="Factor Mining in Quantitative Investing: A Survey"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Factor Mining in Quantitative Investing: A Survey</span>
        </a>
      
    
  </div>

          
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem collapsed
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="AINN"
        id="heading-8cf999be348db55296cba8c2de931572" role="tab" data-toggle="collapse" href="#collapse-8cf999be348db55296cba8c2de931572"
        aria-expanded="false"
      >
        AINN
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse " id="collapse-8cf999be348db55296cba8c2de931572"
           role="tabpanel" aria-labelledby="heading-8cf999be348db55296cba8c2de931572">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/AIBasis-Neural-Networks/" title="AIBasis_Neural_Networks"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AIBasis_Neural_Networks</span>
        </a>
      
    
      
      
        <a href="/posts/AINN-Attention/" title="AINN Attention"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AINN Attention</span>
        </a>
      
    
      
      
        <a href="/posts/AINN-Transformer/" title="AINN Transformer"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AINN Transformer</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem collapsed
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="CS294 LLM Agents"
        id="heading-031f1bdd6c7d4beec5865e47313ba4bf" role="tab" data-toggle="collapse" href="#collapse-031f1bdd6c7d4beec5865e47313ba4bf"
        aria-expanded="false"
      >
        CS294 LLM Agents
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse " id="collapse-031f1bdd6c7d4beec5865e47313ba4bf"
           role="tabpanel" aria-labelledby="heading-031f1bdd6c7d4beec5865e47313ba4bf">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/CS294-1-LLM-Reasoning/" title="CS294-1-LLM-Reasoning"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-1-LLM-Reasoning</span>
        </a>
      
    
      
      
        <a href="/posts/CS294-3-Autogen/" title="CS294-3-Autogen"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-3-Autogen</span>
        </a>
      
    
      
      
        <a href="/posts/Agents-in-Coding-A-survey/" title="Agents in Coding: A Survey"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Agents in Coding: A Survey</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem collapsed
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Torch"
        id="heading-a8b6ce53a1cdf2ee4d3f16b939029b2b" role="tab" data-toggle="collapse" href="#collapse-a8b6ce53a1cdf2ee4d3f16b939029b2b"
        aria-expanded="false"
      >
        Torch
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse " id="collapse-a8b6ce53a1cdf2ee4d3f16b939029b2b"
           role="tabpanel" aria-labelledby="heading-a8b6ce53a1cdf2ee4d3f16b939029b2b">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/Torch-memo/" title="Torch-Memo"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch-Memo</span>
        </a>
      
    
      
      
        <a href="/posts/Torch-Memo-Tensor-Operations/" title="Torch Memo Tensor Operations"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch Memo Tensor Operations</span>
        </a>
      
    
      
      
        <a href="/posts/Torch-Memo-TensorBoard/" title="Torch Memo TensorBoard"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch Memo TensorBoard</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Pre-Training-Is-Dead?</h1>
            
            
              <div class="markdown-body">
                
                <style>
  html, body, .markdown-body {
    font-family: Georgia, sans, serif;
  }
</style>

<h1 id="Neurips-Speeches-Pretraining-is-Dead"><a href="#Neurips-Speeches-Pretraining-is-Dead" class="headerlink" title="Neurips Speeches: Pretraining is Dead?"></a>Neurips Speeches: Pretraining is Dead?</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>I originally intended to write this blog in December, but Ilya’s views faced a lot of opposition at that time. However, in just three months, <strong>the emergence of Deepseek and the explosive growth of the AI agent sector</strong> seem to validate the correctness of this speech. In this blog, I will analyze, from the current perspective, the <strong>shift in the development path of AI starting from scaling laws</strong>, and interpret Ilya’s predictions for future development prospects.</p>
<p>The transcripts are copied from here<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[speech's scripts](https://github.com/shun-liang/readable-talks-transcriptions/blob/main/neurips_2024/Vincent%20Weisser%20-%20.%40ilyasut%20full%20talk%20at%20neurips%202024%20pre-training%20as%20we%20know%20it%20will%20end%20and%20what%20comes%20next%20is%20superintelligence%20agentic%2C%20reasons%2C%20understands%20and%20is%20self%20aware.md)">[2]</span></a></sup>, and you can go through YouTube for the original video<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[the original videos](https://www.youtube.com/watch?v=1yvBqasHLZs&t=10s)">[1]</span></a></sup>.</p>
<h2 id="Table-of-contents"><a href="#Table-of-contents" class="headerlink" title="Table of contents"></a>Table of contents</h2>
    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-b2241494" role="button" aria-expanded="false" aria-controls="collapse-b2241494">
        <div class="fold-arrow">▶</div>TBD
      </div>
      <div class="fold-collapse collapse" id="collapse-b2241494">
        <div class="fold-content">
          <ul><li>0:00:00 <a href="#neurips-then-and-now">NeurIPS Then and Now</a></li><li>0:00:30 <a href="#a-ten-year-retrospective">A Ten Year Retrospective</a></li><li>0:01:22 <a href="#the-scaling-hypothesis-neural-network-history">The Scaling Hypothesis: Neural Network History</a></li><li>0:05:58 <a href="#connectionism-the-foundation-of-deep-learning">Connectionism: The Foundation of Deep Learning</a></li><li>0:09:51 <a href="#scaling-in-biology-body-size-and-brain-mass">Scaling in Biology: Body Size and Brain Mass</a></li><li>0:16:45 <a href="#beyond-neurons-new-frontiers-in-cognition">Beyond Neurons: New Frontiers in Cognition</a></li><li>0:16:59 <a href="#biological-inspiration-in-ai-limited-but-successful">Biological Inspiration in AI: Limited but Successful</a></li><li>0:18:12 <a href="#ai-reasoning-hallucinations-and-rights">AI Reasoning, Hallucinations, and Rights</a></li><li>0:22:13 <a href="#multi-hop-reasoning-and-distribution-generalization">Multi-hop reasoning and distribution generalization</a></li></ul>
        </div>
      </div>
    </div>

<h2 id="A-Ten-Year-Retrospective"><a href="#A-Ten-Year-Retrospective" class="headerlink" title="A Ten Year Retrospective"></a>A Ten Year Retrospective</h2><p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">
And now we've got my experienced, hopefully visor. But here I'd like to talk a little bit about the work itself and maybe a ten year retrospective on it. Because a lot of the things in this work were correct, but some not so much. And we can review them and we can see what happened and how it gently flowed to where we are today.
</p>

<h3 id="The-Scaling-Hypothesis-Neural-Network-History"><a href="#The-Scaling-Hypothesis-Neural-Network-History" class="headerlink" title="The Scaling Hypothesis: Neural Network History"></a>The Scaling Hypothesis: Neural Network History</h3><p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">So let's begin by talking about what we did. And the way we'll do it is by showing slides from the same talk 10 years ago. But the summary of what we did is the following three bullet points.</p>

<ul>
<li><strong>Autoregressive model trained on text</strong>.</li>
<li><strong>Large neural networks</strong>.</li>
<li><strong>Large dataset</strong>.</li>
</ul>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">And what we said here is that if you have a large neural network with 10 layers, then it can do anything that a human being can do in a fraction of a second. 
    </p>

<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic"> Well, if you believe the deep learning dogma, so to say, that artificial neurons and biological neurons are similar, or at least not too different. And you believe that real neurons are slow, then anything that we can do quickly. By we, I mean human beings. I even mean just one human in the entire world. If there is one human in the entire world that can do some task in a fraction of a second, then a 10 layer neural network can do it too. Right? It follows. You just take their connections and you embed them inside your neural net, the artificial one. </p>

<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">If you could go beyond in your layers somehow, then you could do more. But back then, we could only do 10 layers, which is why we emphasized whatever human beings can do in a fraction of a second. </p>

<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">A different slide from the talk. You might be able to recognize that something auto-regressive is going on here. The slide says that if you have an auto-regressive model and it predicts the next token well enough, then it will in fact grab and capture and grasp the correct distribution over sequences that come next. And this was a relatively new thing. It wasn't literally the first ever auto-regressive neural network, but I would argue it was the first auto-regressive neural network where we really believed that if you train it really well, then you will get whatever you want. </p>

<p><img src="https://s1.imagehub.cc/images/2025/03/10/23f27504cf03f87506abbe68dae8a379.png" srcset="/img/loading.gif" lazyload alt="Autoregressive Model"></p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">Now I'm going to show you some ancient history that many of you may have never seen before. It's called the LSTM. To those unfamiliar, an LSTM is the thing that poor deep learning researchers did before Transformers. And it's basically a ResNet, but rotated 90 degrees.</p>

<p><img src="https://s1.imagehub.cc/images/2025/03/10/9b484a8616b8c6ff89034f50417662f8.png" srcset="/img/loading.gif" lazyload alt="The LSTM model"></p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">Another cool feature from that old talk that I want to highlight is that we used parallelization. But not just any parallelization. We used pipelining, as witnessed by this one layer per GPU. Was it wise to pipeline? As we now know, pipelining is not wise. But we were not as wise back then. So we used that and we got a 3.5x speedup using eight GPUs.</p>

<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">And the conclusion slide in some sense, the conclusion slide from the talk from back then is the most important slide because it spelled out what could arguably be the beginning of the scaling hypothesis, right? That if you have a very big data set and you train a very big neural network, then success is guaranteed. </p>

<p><strong>Maybe this is the prototype of Scaling Law</strong>! Without considering the limitation of data and computing resources (at least we have Moore’s Law), the model’s accuracy can be guaranteed by increasing the number of parameters and the training cost. In other world, the more parameters the model have, the more powerful the model it is, if given enough training resources.</p>
<p>This answer is cruel, but somehow effectiveness.</p>
<h2 id="The-age-of-Pre-Training"><a href="#The-age-of-Pre-Training" class="headerlink" title="The age of Pre-Training"></a>The age of Pre-Training</h2><p><img src="https://s1.imagehub.cc/images/2025/03/10/4965aee1f8450caee29f72db0e993283.png" srcset="/img/loading.gif" lazyload alt="The age of pre-training"></p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">But what this led to the age of pre-training. And the age of pre-training is what we might say the GPT-2 model, the GPT-3 model, the scaling laws. And I want to specifically call out my former collaborators, Alec Radford, also Jared Kaplan, Dario Amode, for really making this work. And this is what's been the driver of all of progress, all the progress that we see today. Extra-large neural networks. Extra-ordinarily large neural networks trained on huge data sets. </p>

<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">But pre-training as we know it will <b>unquestionably end</b>. Pre-training will end. Why will it end? Because while compute is growing through better hardware, better algorithms, and larger clusters, all those things keep increasing your compute. All these things keep increasing your compute. The data is not growing because we have but one internet. We have but one internet. You could even say, you could even go as far as to say that data is the fossil fuel of AI. It was like created somehow, and now we use it, and we've achieved peak data, and there will be no more. 

<h2 id="What-comes-next"><a href="#What-comes-next" class="headerlink" title="What comes next?"></a>What comes next?</h2><p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic"> So, here I'll take a bit of liberty to speculate about what comes next. Actually, I don't need to speculate because many people are speculating too. And I'll mention their speculations. You may have heard the phrase <b>agents</b>. It's common. And I'm sure that eventually something will happen. But people feel like something agents is the future. More concretely, but also a little bit vaguely, <b>synthetic data</b>. But what does synthetic data mean? Figuring this out is a big challenge. And I'm sure that different people have all kinds of interesting progress there. And an <b>inference time compute</b>, or maybe what's been most recently, most vividly seen in O1, the O1 model. These are all examples of things, of people trying to figure out what to do after pretraining. And those are all very good things to do.

<ul>
<li><strong>Agents</strong></li>
<li><strong>Synthetic Data</strong></li>
<li><strong>Inference time compute</strong></li>
</ul>
<h3 id="Scaling-in-Biology-Body-Size-and-Brain-Mass"><a href="#Scaling-in-Biology-Body-Size-and-Brain-Mass" class="headerlink" title="Scaling in Biology: Body Size and Brain Mass"></a>Scaling in Biology: Body Size and Brain Mass</h3><p><img src="https://i-blog.csdnimg.cn/direct/1e4533a2f3a142d59da834d0473cb688.png" srcset="/img/loading.gif" lazyload alt="Example from nature"></p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">I want to mention one other example from biology, which I think is really cool.  What that means is that there is a precedent. There is an example of biology figuring out <b>some kind of different scaling</b>. Something clearly is different. So, I think that is cool.

<h3 id="In-the-long-term…"><a href="#In-the-long-term…" class="headerlink" title="In the long term…"></a>In the long term…</h3><h4 id="Super-Intelligence"><a href="#Super-Intelligence" class="headerlink" title="Super Intelligence"></a>Super Intelligence</h4><ul>
<li>Agentic</li>
</ul>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">Those systems are actually going to be agentic in real ways. Whereas right now, the systems are not agents in any meaningful sense. Just very, that might be too strong. They are very, very slightly agentic. Just beginning.

<ul>
<li>Reasons</li>
</ul>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">It will actually reason. And by the way, I want to mention something about reasoning. Is that a system that reasons, the more it reasons, the more unpredictable it becomes. The more it reasons, the more unpredictable it becomes. All the deep learning that we've been used to is very predictable. Because if you've been working on replicating human intuition essentially. It's like the gut feel. If you come back to the 0.1 second reaction time. What kind of processing we do in our brains? Well, it's our intuition. So we've endowed our AIs with some of that intuition. <b>But reasoning, and you're seeing some early signs of that. Reasoning is unpredictable. And one reason to see that is because the chess AIs, the really good ones, are unpredictable to the best human chess players</b>.

 <p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">So we will have to be dealing with AI systems that are incredibly unpredictable. They will understand things from limited data. They will not get confused. All the things which are really big limitations. 
<div class="note note-info">
            <p><strong>In the Q&amp;A part</strong>: About reasoning and hallucinations</p><p>You mentioned reasoning as being one of the core aspects of maybe the modeling in the future. And maybe a differentiator. What we saw in some of the poster sessions is that hallucinations in today’s models, the way we’re analyzing… I mean, maybe you correct me. You’re the expert on this. But the way we’re analyzing whether a model is hallucinating today without… Because we know of the dangers of models not being able to reason, that we’re using a statistical analysis. Let’s say some amount of standard deviations or whatever away from the mean. In the future, wouldn’t it… Do you think that a model given reasoning will be able to correct itself, sort of autocorrect itself? And that will be a core feature of future models so that there won’t be as many hallucinations because the model will recognize when… Maybe that’s too esoteric of a question. But the model will be able to reason and understand when a hallucination is occurring. Does the question make sense?</p><p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic"><b>Yes, the answer is yes! </b>
          </div>


<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic"><b>I'm not saying how, by the way. And I'm not saying when. I'm saying that it will. </b>

<ul>
<li>Self-awareness</li>
</ul>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">And when all those things will happen together with self-awareness. Because why not? Self-awareness is useful. You, ourselves, are parts of our own world models. When all those things come together. We will have systems of radically different qualities and properties that exist today. And of course, they will have incredible and amazing capabilities. But the kind of issues that come up with systems like this. And I'll just leave it as an exercise to imagine. It's very different from what we are used to. 

<h2 id="Q-A-Part"><a href="#Q-A-Part" class="headerlink" title="Q&amp;A Part"></a>Q&amp;A Part</h2><p><strong>Q</strong>:  Now, in 2024, are there other biological structures that are part of human cognition that you think are worth exploring in a similar way or that you’re interested in anyway?</p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">So, the way I'd answer this question is that if you are or someone is a person who has a specific insight about, hey, we are all being extremely silly because clearly the brain does something and we are not. And that's something that can be done and that's something that can be done and they should pursue it. I personally don't. Well, <b>it depends on the level of abstraction you're looking at</b>. Maybe I'll answer it this way. Like there's been a lot of desire to make <b>biologically inspired AI</b>. And you could argue on some level that biologically inspired AI is incredibly successful, which is all of deep learning is biologically inspired AI. But on the other hand, <b>the biological inspiration was very, very, very modest</b>. It's like, let's use neurons. This is the full extent of the biological inspiration. Let's use neurons. And more detailed biological inspiration has been very hard to come by. But I wouldn't rule it out. I think if someone has a special insight, they might be able to see something and that would be useful.


<p><strong>Q</strong>: I wanted to ask, do you think LLMs generalize multi-hop reasoning out of distribution?</p>
<p style="font-family: 'Times New Roman', serif; font-size: 20px; font-style: italic">So, okay. The question assumes that the answer is yes or no. But the question should not be answered yes or no. Because what does it mean, out of distribution generalization? What does it mean? What does it mean in distribution? And what does it mean out of distribution? Because it's a test of time talk, I'll say, that long, long ago, before people were using deep learning, they were using things like string matching, n-grams. For machine translation, people were using statistical phrase tables. Can you imagine? They had tens of thousands of code of complexity, which was, I mean, it was truly unfathomable. And back then, generalization meant, is it literally not in the same word phrasing as in the data set? Now, we may say, well, sure, my model achieves this high score on, I don't know, math competitions. But maybe the math, maybe some discussion in some forum on the internet was about the same ideas, and therefore it's memorized. Well, okay, you could say maybe it's in distribution, maybe it's memorization. But I also think that our standards for what counts as generalization have increased really quite substantially, dramatically, unimaginably, if you keep track. And so I think the answer is, to some degree, probably not as well as human beings. I think it is true that human beings generalize much better. But at the same time, they definitely generalize out of distribution to some degree.

<h2 id="Other-Lectures-about-Pre-training-and-Scaling-Law"><a href="#Other-Lectures-about-Pre-training-and-Scaling-Law" class="headerlink" title="Other Lectures about Pre-training and Scaling Law"></a>Other Lectures about Pre-training and Scaling Law</h2><h3 id="Jason-Wei-Scaling-Paradigms-for-Large-Language-Models"><a href="#Jason-Wei-Scaling-Paradigms-for-Large-Language-Models" class="headerlink" title="Jason Wei: Scaling Paradigms for Large Language Models"></a>Jason Wei: Scaling Paradigms for Large Language Models</h3><p>The original video on Youtube is <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=yhpjpNXJDco">here</a>.<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Jason Wei: Scaling Paradigms for Large Language Models](https://www.youtube.com/watch?v=yhpjpNXJDco)">[5]</span></a></sup></p>
<p>The scaling paradigms for <strong>next-word predictions</strong> works so well, but is it all we need to achieve <strong>AGI</strong>? Maybe not, Jason Wei gives his reasons in the text below:</p>
<p>Some words may be super hard for next-token predictions to implement.</p>
<p><img src="https://s1.imagehub.cc/images/2025/03/17/ce50369c2b6655334d2a218439e9b9c4.png" srcset="/img/loading.gif" lazyload alt="The disadvantage of next-token prediction"></p>
<p>In pure-next-token prediction, the model sees language as a <strong>pure probablility of language distribution</strong>, it means the model uses similar computing resources to solve hard tasks and easier ones.</p>
<p><strong>The approach</strong>: Chain of Thought Prompting.</p>
<p><strong>The limitation of CoT</strong>: We want the model’s chain of thought to be my inner monologue or like a stream of thought.</p>
<p>All in all, only the “dumb” scaling law is not correct in the long run! We need to find something for effective, rather than naive. Maybe is <strong>LLM reasoning</strong>.</p>
<h3 id="Shital-Shah’s-comment-on-scaling-law…"><a href="#Shital-Shah’s-comment-on-scaling-law…" class="headerlink" title="Shital Shah’s comment on scaling law…"></a>Shital Shah’s comment on scaling law…</h3><p><img src="https://s1.imagehub.cc/images/2025/03/25/1bbc71c38c1050629c8a009b2ea4a765.png" srcset="/img/loading.gif" lazyload alt="Scaling Law"></p>
<p>The whole transcripts</p>
<blockquote>
<p>With current synthetic data techniques, one issue is that they don’t add a lot of new entropy to the original pre-training data. Pre-training data is synthesized from spending centuries of human-FLOPs. <strong>Prompt-based synthetic generation can only produce data in the neighborhood of existing data.</strong> This creates an entropy bottleneck: there is simply not enough entropy per token to gain as you move down the tail of organic data or rely on prompt-based synthetic data.</p>
</blockquote>
<blockquote>
<p>A possible solution is to <strong>spend more compute time during testing to generate synthetic data with higher entropy content</strong>. The entropy per token in a given dataset seems to be related to the FLOPs spent on generating that data. Human data was generated from a vast amount of compute spent by humans over millennia. Our pre-training data is the equivalent of fossil fuel, and that data is running out.</p>
</blockquote>
<blockquote>
<p>While human-FLOPs are in limited supply, <strong>GPU-FLOPs through techniques like ttc can allow us to generate synthetic data with high entropy</strong>, offering a way to overcome this bottleneck. However, the bad news is that we will need more compute than predicted by scaling laws. So, can’t we just rely solely on task-targeted compute?</p>
</blockquote>
<blockquote>
<p>Merely scaling inference compute won’t be sufficient. A weak model can spend an inordinate amount of inference compute and still fail to solve a hard problem. <strong>There seems to be an intricate, intertwined dance between training and inference compute, with each improving the other. Imagine a cycle of training a model, generating high-entropy synthetic data by scaling inference compute, and then using that data to continue training. This is the self-improving recipe.</strong></p>
</blockquote>
<blockquote>
<p>Humans operate in a similar way: we consume previously generated data and use it to create new data for the next generation. One critical element in this process is <strong>embodiment</strong>, which <strong>enables the transfer of entropy from our environment</strong>. Spend thousands of years of human-FLOPs in this way, and you get the pre-training data that we currently use!</p>
</blockquote>
<h3 id="Neurips-2024-speech-delivered-by-Kaiming-He"><a href="#Neurips-2024-speech-delivered-by-Kaiming-He" class="headerlink" title="Neurips 2024 speech delivered by Kaiming He"></a>Neurips 2024 speech delivered by Kaiming He</h3><p>ML Research, via the Len</p>
<ul>
<li>Research is SGD in a chaotic landscape</li>
<li>Look for ‘surprise’</li>
<li>Future is the Real Test set</li>
<li>Scalability</li>
</ul>
<p>The files stores in <a target="_blank" rel="noopener" href="https://people.csail.mit.edu/kaiming/neurips2024workshop/neurips2024_newinml_kaiming.pdf">here</a>.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=1yvBqasHLZs&t=10s">the original videos</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://github.com/shun-liang/readable-talks-transcriptions/blob/main/neurips_2024/Vincent%20Weisser%20-%20.%40ilyasut%20full%20talk%20at%20neurips%202024%20pre-training%20as%20we%20know%20it%20will%20end%20and%20what%20comes%20next%20is%20superintelligence%20agentic%2C%20reasons%2C%20understands%20and%20is%20self%20aware.md">speech’s scripts</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.reddit.com/r/singularity/comments/1hdrjvq/ilyas_full_talk_at_neurips_2024_pretraining_as_we/">https://www.reddit.com/r/singularity/comments/1hdrjvq/ilyas_full_talk_at_neurips_2024_pretraining_as_we/</a>
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://luluyan.medium.com/notes-on-ilyas-talk-at-neurips-2024-will-the-causal-compass-be-guiding-ai-s-future-with-reason-e387fa6d8dc9">Remarks by luluyan</a>
<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=yhpjpNXJDco">Jason Wei: Scaling Paradigms for Large Language Models</a>
<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><blockquote>
Scaling laws assume that the quality of tokens remains mostly the same as you scale. However, in real-world large-scale datasets, this is not true. When there is an upper bound on quality training tokens, there is an upper bound on scaling. But what about synthetic data?
</blockquote>
<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://x.com/sytelus/status/1857102074070352290">https://x.com/sytelus/status/1857102074070352290</a>
<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Artificial-Intelligence/" class="category-chain-item">Artificial Intelligence</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Artificial-Intelligence/" class="print-no-link">#Artificial Intelligence</a>
      
        <a href="/tags/Deep-Learning/" class="print-no-link">#Deep Learning</a>
      
        <a href="/tags/Finished/" class="print-no-link">#Finished</a>
      
        <a href="/tags/Pretraining/" class="print-no-link">#Pretraining</a>
      
        <a href="/tags/Large-Language-Models/" class="print-no-link">#Large Language Models</a>
      
        <a href="/tags/Celebrity-speeches/" class="print-no-link">#Celebrity speeches</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Pre-Training-Is-Dead?</div>
      <div>https://xiyuanyang-code.github.io/posts/Pre-training-Is-Dead/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Xiyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 9, 2025</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Updated on</div>
          <div>May 12, 2025</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/Taking-Notes/" title="Taking-Notes">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Taking-Notes</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/CS294-3-Autogen/" title="CS294-3-Autogen">
                        <span class="hidden-mobile">CS294-3-Autogen</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"xiyuanyang-code/xiyuanyang-code.github.io","repo-id":"R_kgDONRhvHQ","category":"Announcements","category-id":"DIC_kwDONRhvHc4ClBnp","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://github.com/xiyuanyang-code" target="_blank" rel="nofollow noopener"><span>YXY</span></a> <a href="https://xiyuanyang-code.github.io/Loving-Count/" target="_blank" rel="nofollow noopener"><span>❤️</span></a> <a href="https://github.com/Siyan-Li" target="_blank" rel="nofollow noopener"><span>LSY</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
