

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/YXY.png">
  <link rel="icon" href="/img/YXY.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#c30000">
  <meta name="author" content="Xiyuan Yang">
  <meta name="keywords" content="Code">
  
    <meta name="description" content="The introduction and explanation for the Basic Neural Network Transformer.">
<meta property="og:type" content="article">
<meta property="og:title" content="AINN Attention">
<meta property="og:url" content="https://xiyuanyang-code.github.io/posts/AINN-Attention/index.html">
<meta property="og:site_name" content="Xiyuan Yang&#39;s Blog">
<meta property="og:description" content="The introduction and explanation for the Basic Neural Network Transformer.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xiyuanyang-code.github.io/img/cover/attention.jpg">
<meta property="article:published_time" content="2025-04-29T11:55:46.000Z">
<meta property="article:modified_time" content="2025-06-24T04:57:32.347Z">
<meta property="article:author" content="Xiyuan Yang">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Neural Networks">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://xiyuanyang-code.github.io/img/cover/attention.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>AINN Attention - Xiyuan Yang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xiyuanyang-code.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4","placement":"left","visible":"hover","icon":"❡"},"progressbar":{"enable":true,"height_px":3,"color":"red","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"L7r0uGb0fafbzNvmBADCMH42-gzGzoHsz","app_key":"2Lr1fQ2rjhwRiUrDx0VOQyUm","server_url":null,"path":"window.location.pathname","ignore_local":true},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Xiyuan Yang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://xiyuanyang-code.github.io/posts/Above-All-en/" target="_self">
                <i class="iconfont icon-bookmark-fill"></i>
                <span>Intro</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archive</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Category</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tag</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://xiyuanyang-code.github.io/resume/" target="_self">
                <i class="iconfont icon-code"></i>
                <span>Resume</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>Else</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/FAQ/" target="_self">
                    <i class="iconfont icon-bug"></i>
                    <span>FAQ</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/links" target="_self">
                    <i class="iconfont icon-link-fill"></i>
                    <span>Links</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/Blog-word-counting/" target="_self">
                    <i class="iconfont icon-plan"></i>
                    <span>Status</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/posts/My-Posts/" target="_self">
                    <i class="iconfont icon-notebook"></i>
                    <span>All Posts</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/Recordings/" target="_self">
                    <i class="iconfont icon-clipcheck"></i>
                    <span>Daily Loggings</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://github.com/xiyuanyang-code" target="_self">
                    <i class="iconfont icon-github-fill"></i>
                    <span>My Github</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/" target="_self">
                    <i class="iconfont icon-copyright"></i>
                    <span>About Hexo</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/posts/Life-musings/" target="_self">
                    <i class="iconfont icon-brush"></i>
                    <span>Life Musing</span>
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/place.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="AINN Attention"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Xiyuan Yang
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-29 19:55" pubdate>
          April 29, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.3k words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Artificial Intelligence"
        id="heading-5cd2adc9e2a5254e4c1da803519f298b" role="tab" data-toggle="collapse" href="#collapse-5cd2adc9e2a5254e4c1da803519f298b"
        aria-expanded="true"
      >
        Artificial Intelligence
        <span class="list-group-count">(17)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-5cd2adc9e2a5254e4c1da803519f298b"
           role="tabpanel" aria-labelledby="heading-5cd2adc9e2a5254e4c1da803519f298b">
        
        
          
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/Pre-training-Is-Dead/" title="Pre-Training-Is-Dead?"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Pre-Training-Is-Dead?</span>
        </a>
      
    
      
      
        <a href="/posts/Deep-Learning-Memo/" title="Deep_Learning_Memo"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Deep_Learning_Memo</span>
        </a>
      
    
      
      
        <a href="/posts/RL-speeches/" title="RL_speeches"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">RL_speeches</span>
        </a>
      
    
      
      
        <a href="/posts/AI-Paper-2024/" title="AI-Paper-2024"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AI-Paper-2024</span>
        </a>
      
    
      
      
        <a href="/posts/RAG-tutorial/" title="RAG-Tutorial"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">RAG-Tutorial</span>
        </a>
      
    
      
      
        <a href="/posts/Imagenet/" title="ImageNet and ILSVRC"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">ImageNet and ILSVRC</span>
        </a>
      
    
      
      
        <a href="/posts/LLM-Evaluating/" title="LLM Evaluating"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">LLM Evaluating</span>
        </a>
      
    
      
      
        <a href="/posts/Factor-Mining-in-Quantitative-Investing-A-Survey/" title="Factor Mining in Quantitative Investing: A Survey"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Factor Mining in Quantitative Investing: A Survey</span>
        </a>
      
    
  </div>

          
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="AINN"
        id="heading-8cf999be348db55296cba8c2de931572" role="tab" data-toggle="collapse" href="#collapse-8cf999be348db55296cba8c2de931572"
        aria-expanded="true"
      >
        AINN
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-8cf999be348db55296cba8c2de931572"
           role="tabpanel" aria-labelledby="heading-8cf999be348db55296cba8c2de931572">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/AIBasis-Neural-Networks/" title="AIBasis_Neural_Networks"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AIBasis_Neural_Networks</span>
        </a>
      
    
      
      
        <a href="/posts/AINN-Attention/" title="AINN Attention"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">AINN Attention</span>
        </a>
      
    
      
      
        <a href="/posts/AINN-Transformer/" title="AINN Transformer"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AINN Transformer</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem collapsed
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="CS294 LLM Agents"
        id="heading-031f1bdd6c7d4beec5865e47313ba4bf" role="tab" data-toggle="collapse" href="#collapse-031f1bdd6c7d4beec5865e47313ba4bf"
        aria-expanded="false"
      >
        CS294 LLM Agents
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse " id="collapse-031f1bdd6c7d4beec5865e47313ba4bf"
           role="tabpanel" aria-labelledby="heading-031f1bdd6c7d4beec5865e47313ba4bf">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/CS294-1-LLM-Reasoning/" title="CS294-1-LLM-Reasoning"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-1-LLM-Reasoning</span>
        </a>
      
    
      
      
        <a href="/posts/CS294-3-Autogen/" title="CS294-3-Autogen"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-3-Autogen</span>
        </a>
      
    
      
      
        <a href="/posts/Agents-in-Coding-A-survey/" title="Agents in Coding: A Survey"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Agents in Coding: A Survey</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
    
    
    
    <div class="category-sub row nomargin-x">
      <a class="category-subitem collapsed
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Torch"
        id="heading-a8b6ce53a1cdf2ee4d3f16b939029b2b" role="tab" data-toggle="collapse" href="#collapse-a8b6ce53a1cdf2ee4d3f16b939029b2b"
        aria-expanded="false"
      >
        Torch
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse " id="collapse-a8b6ce53a1cdf2ee4d3f16b939029b2b"
           role="tabpanel" aria-labelledby="heading-a8b6ce53a1cdf2ee4d3f16b939029b2b">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/Torch-memo/" title="Torch-Memo"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch-Memo</span>
        </a>
      
    
      
      
        <a href="/posts/Torch-Memo-Tensor-Operations/" title="Torch Memo Tensor Operations"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch Memo Tensor Operations</span>
        </a>
      
    
      
      
        <a href="/posts/Torch-Memo-TensorBoard/" title="Torch Memo TensorBoard"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Torch Memo TensorBoard</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">AINN Attention</h1>
            
            
              <div class="markdown-body">
                
                <style>
  html, body, .markdown-body {
    font-family: Georgia, sans, serif;
  }
</style>

<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>由于笔者发现在讲解Transformer的网络架构的时候缺乏对前置知识的细致理解。走马观花式的学习终究还是假学习。</p>
<ul>
<li>为什么RNN的Encoder-Decoder架构的表现不好？</li>
<li>为什么Transformer需要设计成矩阵？</li>
<li>Q, K, V究竟代表什么意思？</li>
</ul>
<p>因此，笔者下决心<strong>重起炉灶重构博客</strong>，尝试啃下这一块硬骨头。</p>
<p>今天的博客将会聚焦于<strong>Attention Mechanism</strong>的思想和数学实现。</p>
<h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul>
<li><strong>注意力机制</strong></li>
<li><strong>RNN和seq2seq</strong></li>
<li><strong>seq2seq</strong>和<strong>RNN</strong>的结合</li>
</ul>
<h2 id="Attention-Mechanism-1"><a href="#Attention-Mechanism-1" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h2><p>注意力机制是从生物学上获得的启发。应用到视觉世界中，威廉·詹姆斯提出了<strong>双组件框架</strong>，即人通过<strong>自主性提示和非自主性提示</strong>来选择性地引导注意力的焦点。</p>
<h3 id="非自主性提示和自主性提示"><a href="#非自主性提示和自主性提示" class="headerlink" title="非自主性提示和自主性提示"></a>非自主性提示和自主性提示</h3><ul>
<li>非自主性提示：<strong>“显眼”的物品</strong>，视觉上最敏锐的部分，获得直接的感官感受。</li>
<li>自主性提示：<strong>受到认知和意识</strong>的控制。</li>
</ul>
<div class="note note-info">
            <p><img src="https://s1.imagehub.cc/images/2025/04/29/77c4375efb4078e24a28fa9e6d7c9723.png" srcset="/img/loading.gif" lazyload alt="macbook apple"></p><p>例如，</p><ul><li>我因为Macbook图标很亮眼而注意到它：非自主性提示。（直接的感官感受）</li><li>我因为想到要去买苹果而注意到桌上的苹果。（自主性提示，收到自我意识的驱动）</li></ul>
          </div>

<h3 id="查询，键和值"><a href="#查询，键和值" class="headerlink" title="查询，键和值"></a>查询，键和值</h3><p>如果建模这两种注意力机制？这是神经科学家非常关心的问题。</p>
<p>显然，对于<strong>非自主性提示</strong>，本质就是<strong>图片的特征提取</strong>，我们可以使用卷积核和卷积神经网络来提取图片中的特征（例如高亮度区域，色彩鲜艳的区域，锐度高的区域等等），不是我们今天讨论的重点。</p>
<p>但是对于<strong>自主性提示</strong>，其机制相对更加复杂。并且在现实世界中注意力往往是<strong>两种提示相互夹杂</strong>的。我们可以使用如下的基本结构来进行建模：</p>
<p><img src="https://zh.d2l.ai/_images/qkv.svg" srcset="/img/loading.gif" lazyload alt="动手学深度学习 注意力机制"></p>
<p>对于人类一般性的行为，我们可以建模成：</p>
<p>$$\boxed{\text{attention}} \to \boxed{\text{sensory input}} \to \boxed{\text{output}}$$</p>
<blockquote>
<p>视觉上产生了<strong>注意力</strong>（自主 &amp; 非自主），接着在大脑皮层产生感觉，并做出对应的输出。</p>
<ul>
<li>翻译：接受视觉输入 $\to$ 在脑海中产生感觉（<strong>思考的过程</strong>） $\to$ 输出成另一种语言</li>
</ul>
</blockquote>
<p>当我们看到一幅画中的苹果，可能会联想到吃苹果的甜味，甚至不自觉流口水。这一过程可以用<strong>注意力机制</strong>来解释，涉及三个核心概念：</p>
<ol>
<li><strong>查询（Query）</strong>：当前的感知输入，如画中苹果的视觉特征（颜色、形状等），它触发大脑的检索机制（<strong>就是非自主性提示</strong>）。</li>
<li><strong>键（Key）</strong>：长期记忆中的关联信息，如过去吃苹果的经验。大脑会计算<strong>Query</strong>和<strong>Key</strong>的匹配程度，选择最相关的记忆。</li>
<li><strong>值（Value）</strong>：被选中的<strong>Key</strong>对应的具体信息，如“苹果是甜的”，进而触发味觉联想和生理反应。</li>
</ol>
<p>这一机制的关键在于：</p>
<ul>
<li><strong>选择性</strong>：并非所有记忆都会被激活，只有与当前输入最相关的信息才会被提取。</li>
<li><strong>联想性</strong>：视觉输入（Query）通过匹配记忆（Key）触发跨模态体验（Value），如“看到苹果→想到甜味”。</li>
<li><strong>自动化</strong>：该过程通常是快速、无意识的，体现大脑的高效信息检索能力。</li>
</ul>
<p>通过这种机制，外部刺激（如画面）能自动激活相关记忆，并影响认知和生理反应。</p>
<div class="note note-primary">
            <p>在注意力机制中，自主性提示被称为<em>查询</em>（query），而非自主性提示（客观存在的咖啡杯和书本）作为<em>键</em>（key）与感官输入（sensory inputs）的<em>值</em>（value）构成一组 pair 作为输入。而给定任何查询，注意力机制通过<em>注意力汇聚</em>（attention pooling）将非自主性提示的 key 引导至感官输入。</p><ul><li>例如在RNN中，隐藏状态H是自主性提示，而新的input就是非自主性提示。<ul><li>如果不更新隐藏状态，而是直接对序列进行暴力建模，就相当于<strong>只考虑到非自主性提示</strong>，就是经典的MLP！</li></ul></li></ul>
          </div>

<p>来点抽象的！我们给定键值对$(x_i, y_i)$，并给出需要查询的$x_q$(query)，我们需要输出$\hat{y}&#x3D;f(x_q)$作为我们的输出值。</p>
<p>$f$就是注意力机制的黑箱函数，给定好框架之后的创新无非就是针对对于$f$的函数的设计。</p>
<h3 id="注意力汇聚算法"><a href="#注意力汇聚算法" class="headerlink" title="注意力汇聚算法"></a>注意力汇聚算法</h3><p>我们不妨设计一些简单的$f$，来看看效果怎么样。最简单的设计就是<strong>平均估计个体</strong>，即使用<strong>Average Pooling</strong>的操作：</p>
<p>$$f(x_q) &#x3D; \frac{1}{n} \sum_{i&#x3D; 1}^{n}y_i$$</p>
<p>这细细一想就很扯淡，因为<strong>这样无论给出什么样的query</strong>输出的结果总是一样的，bullshit！</p>
<p>也就是说，我们<strong>希望在输出预测的时候尽可能的使用到已知的信息</strong>，即我们所知道的键值对$(x_i, y_i)$。一个很自然的想法是**判断$x_q$和每一个键$x_i$**的相关程度，并以此为基础作为权重，再加权平均所有的$y_i$。</p>
<div class="note note-success">
            <p>例如我从Query为“画中的苹果”检索到脑海中的苹果，可以认为是<strong>这两个事物</strong>的相关性很强，因此权重非常大，最后的输出就几乎全是“脑海中的苹果”对应的值。如果我的query是“画中那个洒满椒盐的苹果”，那可能这和脑海中“椒盐”这个键的相关性就会大幅度上升，进而产生相对应（值）的咸咸的感觉。</p>
          </div>

<p>我们便得到了<strong>注意力汇聚公式</strong>（Attention Pooling），本质上就是加一个与键有关的权重，没什么特别的：</p>
<p>$$f(x_q) &#x3D; \sum_{i&#x3D; 1}^{n} \alpha(x_q, x_i) y_i,\ \sum_{i &#x3D; 1}^{n}\alpha(x_q, x_i) &#x3D; 1 $$</p>
<p>如何设计权重？<strong>考虑相关性</strong>，因此我们可以对$\alpha(x_q, x_i)$进一步细化，我们假设$K(x_i,x_j)$衡量了两个向量的相关性：</p>
<p>$$\alpha(x_q, x_i) &#x3D; \frac{K(x_q, x_i)}{\sum_{j &#x3D; 1}^{n}K(x_q, x_j)}$$</p>
<p>更进一步，如果我们考虑高斯核公式（先考虑一阶的）：</p>
<p>$$\alpha(x_q,x_i) &#x3D; \frac{\exp(-\frac{1}{2}(x_q - x_i)^2)}{\sum_{j &#x3D; 1}^{n}\exp(-\frac{1}{2}(x_q - x_j)^2)} &#x3D; \text{softmax}(-\frac{1}{2}(x_q - x_i)^2)$$</p>
<p>那我们就可以得到一个具体化的$f(x)$：</p>
<p>$$\hat{y}  &#x3D; f(x) &#x3D; \sum_{i &#x3D; 1}^{n} \text{softmax}(-\frac{1}{2}(x_q - x_i)^2) y_i$$</p>
<p>更进一步，我们希望这个函数可以携带可被学习的参数：</p>
<p>$$\hat{y}  &#x3D; f(x) &#x3D; \sum_{i &#x3D; 1}^{n} \text{softmax}(-\frac{1}{2}((x_q - x_i)\omega)^2) y_i$$</p>
<p><img src="https://s1.imagehub.cc/images/2025/05/02/e2a1a67fce86e92b82ffb35760110ca4.png" srcset="/img/loading.gif" lazyload alt="实验"></p>
<p>热力图显示当key和query越接近的时候，其权重更高。（这里是一维的情况，就直接比较两个值的大小）</p>
<p>但是带权重的注意力汇聚在做梯度下降的时候<strong>会出现不平滑的现象</strong>（虽然loss确实下降了），因为在键值对很小的情况下很容易出现过拟合的情况。</p>
<p><img src="https://s1.imagehub.cc/images/2025/05/02/db8dfddacbe453aa0986f403f68364a1.png" srcset="/img/loading.gif" lazyload alt="Overfitting"></p>
<blockquote>
<p>可以理解为在一些有噪声的点附近，由于过拟合的存在（额外参数的影响）让<strong>这个点的权重变大了</strong>，导致heatmap出现了断点处的偏移。</p>
</blockquote>
<p>这里引用一个非常好的回答：<a target="_blank" rel="noopener" href="https://discuss.d2l.ai/t/nadaraya-watson/5760/4">参数化的意义是什么</a></p>
<p><strong>增加权重的意义在于使得对于远距离的key有着更小的关注，缩小的注意力的范围。</strong></p>
<h4 id="Adding-Batches"><a href="#Adding-Batches" class="headerlink" title="Adding Batches"></a>Adding Batches</h4><p>为了充分发挥GPU的并行优势，我们可以使用<strong>批量矩阵的乘法</strong>。</p>
<p>假设现在有$n$个$(a,b)$的二维矩阵$X_i(1\le i \le n)$和$n$个$(b,c)$的二维矩阵$Y_i(1\le i \le n)$，在非并行的状态下，我们需要做$n$次顺序矩阵乘法，即$X_i \times Y_i:&#x3D;A_i$，最终得到$n$个$(a,c)$的矩阵。</p>
<p><strong>并行</strong>的本质就是<strong>大家一起做运算</strong>，把$n$个$(a,b)$的二维矩阵$X_i(1\le i \le n)$可以定义为<strong>三维张量</strong>$T$，size是$(n,a,b)$。</p>
<p>因此，**假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$**。</p>
<p>如何使用小批量乘法来改写注意力机制？</p>
<p>计算加权平均值的过程可以看做是两个向量的内积操作（需要归一化），因此假设每一次批量是size为$n$，那么：</p>
<p>$$\text{Weight(n, 1, len)} \times \text{Value(n, len, 1)} &#x3D; \text{Score(n,1)}$$</p>
<blockquote>
<p>$n$在实际情况下可以定义为查询的个数，$len$是键值对的个数。</p>
<p>很惊讶的发现在这里$len$和$n$是无关的量，即我们可以不断的scale up键值对的个数，即获得更多的采样。</p>
</blockquote>
<p>批量矩阵乘法在这里<strong>可以计算小批量数据的加权平均值</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># orginal weights and values</span><br>weights = torch.ones((<span class="hljs-number">2</span>,<span class="hljs-number">10</span>)) * <span class="hljs-number">0.1</span><br>values = torch.arange(<span class="hljs-number">20.0</span>).reshape((<span class="hljs-number">2</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># weight: torch.Size([2, 10])</span><br><span class="hljs-comment"># values: torch.Size([2, 10])</span><br><br><span class="hljs-comment"># compute in sequential order</span><br>all_answer = torch.Tensor()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,weights.shape[<span class="hljs-number">0</span>]):<br>    weights_single = weights[i]<br>    values_single = values[i]<br>    answer = weights_single.dot(values_single.T).unsqueeze(-<span class="hljs-number">1</span>)<br>    all_answer = torch.cat([all_answer, answer])<br><br><span class="hljs-comment"># custom the size</span><br>all_answer1 = all_answer.unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># using torch.bmm</span><br>weights = weights.unsqueeze(<span class="hljs-number">1</span>)<br><span class="hljs-comment"># weights: [2,1,10]</span><br>values = values.unsqueeze(-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># values: [2,10,1]</span><br>all_answer2 = torch.bmm(weights, values)<br><br><span class="hljs-keyword">assert</span> torch.allclose(all_answer1, all_answer2)<br></code></pre></td></tr></table></figure>

<h4 id="Code-Nadaraya-Waston-Kernel"><a href="#Code-Nadaraya-Waston-Kernel" class="headerlink" title="Code: Nadaraya-Waston Kernel"></a>Code: Nadaraya-Waston Kernel</h4><p>在这个部分，我们将要实现最基本的注意力汇聚算法来尝试拟合一个函数，数学原理如上文所呈现：</p>
<p>不带参数的版本：</p>
<p>$$\hat{y}  &#x3D; f(x) &#x3D; \sum_{i &#x3D; 1}^{n} \text{softmax}(-\frac{1}{2}(x_q - x_i)^2) y_i$$</p>
<p>带参数的版本：</p>
<p>$$\hat{y}  &#x3D; f(x) &#x3D; \sum_{i &#x3D; 1}^{n} \text{softmax}(-\frac{1}{2}((x_q - x_i)\omega_i)^2) y_i$$</p>
<blockquote>
<p>注意，这里和书上的公式有点不太一样，书上只引入了一个标量参数$w$，导致非常容易出现过拟合的现象，这里的可学习参数$\vec{\mathbf{w}} &#x3D; \{w_1, w_2, w_3,\dots,w_n\}$，其中$n$是预先定义好的<strong>键值对的个数</strong>。</p>
</blockquote>
<p>代码如下，主要的核心逻辑是：</p>
<ul>
<li>生成数据集，我们需要拟合的函数是$f(x) &#x3D; 2\sin(x) + 0.4\sin(3x) + 0.6\sin(6x) + \sqrt{x}$</li>
<li>首先直接根据不带参数的版本，即没有任何的可训练参数，直接生成test的结果。</li>
<li>接下来使用带参数的版本进行梯度下降训练，得到第二个拟合结果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;0,1,2,3&quot;</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_heatmaps</span>(<span class="hljs-params"></span><br><span class="hljs-params">    matrices,</span><br><span class="hljs-params">    xlabel=<span class="hljs-string">&quot;Keys&quot;</span>,</span><br><span class="hljs-params">    ylabel=<span class="hljs-string">&quot;Queries&quot;</span>,</span><br><span class="hljs-params">    titles=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    figsize=(<span class="hljs-params"><span class="hljs-number">6</span>, <span class="hljs-number">6</span></span>),</span><br><span class="hljs-params">    cmap=<span class="hljs-string">&quot;Reds&quot;</span>,</span><br><span class="hljs-params">    save_path=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Display heatmaps for attention weights or other matrices.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        matrices: Input tensor or array of shape (num_rows, num_cols, height, width)</span><br><span class="hljs-string">        xlabel: Label for x-axis</span><br><span class="hljs-string">        ylabel: Label for y-axis</span><br><span class="hljs-string">        titles: List of titles for subplots</span><br><span class="hljs-string">        figsize: Size of the figure</span><br><span class="hljs-string">        cmap: Color map</span><br><span class="hljs-string">        save_path: Path to save the figure (None for not saving)</span><br><span class="hljs-string">        show: Whether to display the figure</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Convert PyTorch tensor to numpy array if needed</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(matrices, <span class="hljs-string">&quot;detach&quot;</span>):<br>        matrices = matrices.detach().cpu().numpy()<br><br>    num_rows, num_cols = matrices.shape[<span class="hljs-number">0</span>], matrices.shape[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># Create figure and axes</span><br>    fig, axes = plt.subplots(<br>        num_rows, num_cols, figsize=figsize, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>, squeeze=<span class="hljs-literal">False</span><br>    )<br><br>    <span class="hljs-comment"># Plot each matrix</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_rows):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_cols):<br>            pcm = axes[i, j].imshow(matrices[i, j], cmap=cmap)<br><br>            <span class="hljs-comment"># Add labels only to the bottom row and leftmost column</span><br>            <span class="hljs-keyword">if</span> i == num_rows - <span class="hljs-number">1</span>:<br>                axes[i, j].set_xlabel(xlabel)<br>            <span class="hljs-keyword">if</span> j == <span class="hljs-number">0</span>:<br>                axes[i, j].set_ylabel(ylabel)<br><br>            <span class="hljs-comment"># Add titles if provided</span><br>            <span class="hljs-keyword">if</span> titles <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                axes[i, j].set_title(titles[j])<br><br>    fig.savefig(save_path, dpi=<span class="hljs-number">300</span>, bbox_inches=<span class="hljs-string">&quot;tight&quot;</span>)<br><br>    <span class="hljs-comment"># Close the figure to prevent memory leaks</span><br>    plt.close()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_kernel_reg</span>(<span class="hljs-params"></span><br><span class="hljs-params">    y_hat, x_test, y_truth, x_train, y_train, save_path=<span class="hljs-string">&quot;img/kernel_regression.png&quot;</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Plot kernel regression results and save the figure.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        y_hat: Predicted values (tensor or array)</span><br><span class="hljs-string">        x_test: Test input values</span><br><span class="hljs-string">        y_truth: Ground truth values for test inputs</span><br><span class="hljs-string">        x_train: Training input values</span><br><span class="hljs-string">        y_train: Training target values</span><br><span class="hljs-string">        save_path: Path to save the figure (default: &quot;img/kernel_regression.png&quot;)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Convert tensors to numpy if needed</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(y_hat, <span class="hljs-string">&quot;detach&quot;</span>):<br>        y_hat = y_hat.detach().cpu().numpy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(y_truth, <span class="hljs-string">&quot;detach&quot;</span>):<br>        y_truth = y_truth.detach().cpu().numpy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(x_test, <span class="hljs-string">&quot;detach&quot;</span>):<br>        x_test = x_test.detach().cpu().numpy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(x_train, <span class="hljs-string">&quot;detach&quot;</span>):<br>        x_train = x_train.detach().cpu().numpy()<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(y_train, <span class="hljs-string">&quot;detach&quot;</span>):<br>        y_train = y_train.detach().cpu().numpy()<br><br>    <span class="hljs-comment"># Create figure</span><br>    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br><br>    <span class="hljs-comment"># Plot truth and prediction lines</span><br>    plt.plot(x_test, y_truth, label=<span class="hljs-string">&quot;Truth&quot;</span>)<br>    plt.plot(x_test, y_hat, label=<span class="hljs-string">&quot;Pred&quot;</span>)<br><br>    <span class="hljs-comment"># Plot training data points</span><br>    plt.scatter(x_train, y_train, marker=<span class="hljs-string">&quot;o&quot;</span>, alpha=<span class="hljs-number">0.5</span>, s=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;Training data&quot;</span>)<br><br>    <span class="hljs-comment"># Add labels and legend</span><br>    plt.xlabel(<span class="hljs-string">&quot;x&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;y&quot;</span>)<br>    plt.title(<span class="hljs-string">&quot;Kernel Regression Results&quot;</span>)<br>    plt.legend()<br><br>    <span class="hljs-comment"># Save figure with high quality</span><br>    plt.savefig(save_path, dpi=<span class="hljs-number">300</span>, bbox_inches=<span class="hljs-string">&quot;tight&quot;</span>)<br>    plt.close()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_datasets</span>(<span class="hljs-params">width: <span class="hljs-built_in">float</span>, n_train: <span class="hljs-built_in">int</span>, n_test: <span class="hljs-built_in">int</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate random datasets for NWKernel regression.&quot;&quot;&quot;</span><br>    x_train = torch.sort(torch.rand(n_train) * width).values<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">target_function</span>(<span class="hljs-params">x</span>):<br>        <span class="hljs-keyword">return</span> (<br>            <span class="hljs-number">2</span> * torch.sin(x) + <span class="hljs-number">0.4</span> * torch.sin(<span class="hljs-number">3</span> * x) + <span class="hljs-number">0.6</span> * torch.sin(<span class="hljs-number">6</span> * x) + x**<span class="hljs-number">0.5</span><br>        )<br><br>    y_train = target_function(x_train) + torch.normal(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, (n_train,))<br>    x_test = torch.linspace(<span class="hljs-number">0</span>, width, n_test)<br>    y_truth = target_function(x_test)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated datasets - Train: <span class="hljs-subst">&#123;n_train&#125;</span>, Test: <span class="hljs-subst">&#123;n_test&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_truth<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NWKernelRegression</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_train, y_train</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;x_train&quot;</span>, x_train)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;y_train&quot;</span>, y_train)<br>        <span class="hljs-comment"># We use one weight per training example</span><br>        <span class="hljs-variable language_">self</span>.w = nn.Parameter(torch.ones(<span class="hljs-built_in">len</span>(x_train)), requires_grad=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries</span>):<br>        queries = queries.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [n_queries, 1]</span><br>        keys = <span class="hljs-variable language_">self</span>.x_train.unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># [1, n_train]</span><br>        diff = queries - keys  <span class="hljs-comment"># [n_queries, n_train]</span><br>        <span class="hljs-variable language_">self</span>.attention_weights = F.softmax(-((diff * <span class="hljs-variable language_">self</span>.w) ** <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> torch.matmul(<span class="hljs-variable language_">self</span>.attention_weights, <span class="hljs-variable language_">self</span>.y_train)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">    net, x_test, x_train, num_points=<span class="hljs-number">3</span>, save_path=<span class="hljs-string">&quot;img/visualize_attention.png&quot;</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Visualize attention weights for a few test points.&quot;&quot;&quot;</span><br><br>    idxs = torch.linspace(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x_test) - <span class="hljs-number">1</span>, num_points).long()<br>    queries = x_test[idxs]<br>    keys = x_train<br>    w_cpu = net.w.detach().cpu()<br>    keys_cpu = keys.detach().cpu() <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(keys, <span class="hljs-string">&quot;detach&quot;</span>) <span class="hljs-keyword">else</span> torch.tensor(keys)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> i, query <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(queries):<br>            query_cpu = (<br>                query.detach().cpu()<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(query, <span class="hljs-string">&quot;detach&quot;</span>)<br>                <span class="hljs-keyword">else</span> torch.tensor(query)<br>            )<br>            diff = query_cpu - keys_cpu<br>            attn = torch.softmax(-(diff * w_cpu).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>) / <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>            plt.figure()<br>            plt.title(<span class="hljs-string">f&quot;Attention for test x=<span class="hljs-subst">&#123;query_cpu.item():<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br>            plt.plot(keys_cpu, attn.numpy(), <span class="hljs-string">&quot;o-&quot;</span>)<br>            plt.xlabel(<span class="hljs-string">&quot;x_train&quot;</span>)<br>            plt.ylabel(<span class="hljs-string">&quot;Attention weight&quot;</span>)<br>            plt.savefig(save_path)<br>            plt.close()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_kernel_shape</span>(<span class="hljs-params">net, x_train, save_path=<span class="hljs-string">&quot;img/kernelshape_visualize.png&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Visualize the learned kernel shape centered at a point.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>    center = x_train[<span class="hljs-built_in">len</span>(x_train) // <span class="hljs-number">2</span>]<br>    diffs = torch.linspace(-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">100</span>)<br>    w_mean_cpu = net.w.mean().detach().cpu()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        attn = torch.softmax(-(diffs * w_mean_cpu).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>) / <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>    plt.figure()<br>    plt.title(<span class="hljs-string">&quot;Learned Kernel Shape&quot;</span>)<br>    plt.plot(diffs.numpy(), attn.numpy())<br>    plt.xlabel(<span class="hljs-string">&quot;x - center&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;Kernel value&quot;</span>)<br>    plt.savefig(save_path)<br>    plt.close()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_training_process</span>(<span class="hljs-params"></span><br><span class="hljs-params">    record_epoch, record_loss, save_path=<span class="hljs-string">&quot;img/visualize_training_process.png&quot;</span></span><br><span class="hljs-params"></span>):<br>    plt.figure()<br>    plt.title(<span class="hljs-string">&quot;Training Process&quot;</span>)<br>    plt.plot(record_epoch, record_loss)<br>    plt.xlabel(<span class="hljs-string">&quot;epoches&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;Training Loss&quot;</span>)<br>    plt.savefig(save_path)<br>    plt.close()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">width, epochs, n_train, n_test, x_train, y_train, x_test, y_truth</span>):<br>    <span class="hljs-comment"># Move data to GPU if available</span><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    x_train = x_train.to(device)<br>    y_train = y_train.to(device)<br>    x_test = x_test.to(device)<br>    y_truth = y_truth.to(device)<br><br>    <span class="hljs-comment"># Initialize model and optimizer</span><br>    net = NWKernelRegression(x_train, y_train).to(device)<br><br>    <span class="hljs-keyword">if</span> torch.cuda.device_count() &gt; <span class="hljs-number">1</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Using <span class="hljs-subst">&#123;torch.cuda.device_count()&#125;</span> GPUs for DataParallel.&quot;</span>)<br>        net = nn.DataParallel(net)<br><br>    criterion = nn.MSELoss()<br>    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.5</span>)<br>    record_loss = []<br>    record_epochs = []<br><br>    plot_epochs = &#123;<br>        <span class="hljs-number">10</span>,<br>        <span class="hljs-number">100</span>,<br>        <span class="hljs-number">1000</span>,<br>        <span class="hljs-number">5000</span>,<br>        <span class="hljs-number">10000</span>,<br>        <span class="hljs-number">20000</span>,<br>        <span class="hljs-number">50000</span>,<br>        <span class="hljs-number">60000</span>,<br>        <span class="hljs-number">70000</span>,<br>        <span class="hljs-number">80000</span>,<br>        <span class="hljs-number">100000</span>,<br>        <span class="hljs-number">110000</span>,<br>        <span class="hljs-number">120000</span>,<br>    &#125;<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        optimizer.zero_grad()<br>        y_pred = net(x_train)<br>        loss = criterion(y_pred, y_train)<br>        loss.backward()<br>        optimizer.step()<br><br>        record_loss.append(loss.item())<br>        record_epochs.append(epoch)<br><br>        <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch&#125;</span>, Loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) <span class="hljs-keyword">in</span> plot_epochs:<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                y_hat = (<br>                    net.module(x_test)<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.DataParallel)<br>                    <span class="hljs-keyword">else</span> net(x_test)<br>                )<br>            plot_kernel_reg(<br>                y_hat.cpu(),<br>                x_test.cpu(),<br>                y_truth.cpu(),<br>                x_train.cpu(),<br>                y_train.cpu(),<br>                save_path=<span class="hljs-string">f&quot;img/kernel_regression_epoch<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>.png&quot;</span>,<br>            )<br><br>    visualize_training_process(record_epochs, record_loss)<br><br>    <span class="hljs-comment"># Testing</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        y_hat = net.module(x_test) <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.DataParallel) <span class="hljs-keyword">else</span> net(x_test)<br><br>    model_for_vis = net.module <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.DataParallel) <span class="hljs-keyword">else</span> net<br>    plot_kernel_reg(<br>        y_hat.cpu(), x_test.cpu(), y_truth.cpu(), x_train.cpu(), y_train.cpu()<br>    )<br>    visualize_attention(model_for_vis, x_test.cpu(), x_train.cpu())<br>    visualize_kernel_shape(model_for_vis, x_train.cpu())<br><br>    show_heatmaps(<br>        model_for_vis.attention_weights.unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>).cpu(),<br>        xlabel=<span class="hljs-string">&quot;training inputs&quot;</span>,<br>        ylabel=<span class="hljs-string">&quot;testing inputs&quot;</span>,<br>        titles=<span class="hljs-string">&quot;HeatMaps for the final attention&quot;</span>,<br>        save_path=<span class="hljs-string">&quot;img/heatmap_params.png&quot;</span><br>    )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">singleNWKernel</span>(<span class="hljs-params">width, n_train, n_test, x_train, y_train, x_test, y_truth</span>):<br>    sigma = <span class="hljs-number">1.0</span>  <span class="hljs-comment"># fixed kernel size</span><br>    x_train_cpu = x_train.cpu()<br>    y_train_cpu = y_train.cpu()<br>    x_test_cpu = x_test.cpu()<br>    y_truth_cpu = y_truth.cpu()<br><br>    <span class="hljs-comment"># compute attention weights</span><br>    queries = x_test_cpu.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [n_test, 1]</span><br>    keys = x_train_cpu.unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># [1, n_train]</span><br>    diff = queries - keys  <span class="hljs-comment"># [n_test, n_train]</span><br>    attn = torch.softmax(-((diff / sigma) ** <span class="hljs-number">2</span>) / <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 固定sigma</span><br>    y_hat = torch.matmul(attn, y_train_cpu)<br><br>    plot_kernel_reg(<br>        y_hat,<br>        x_test_cpu,<br>        y_truth_cpu,<br>        x_train_cpu,<br>        y_train_cpu,<br>        save_path=<span class="hljs-string">&quot;img/kernel_regression_noparam.png&quot;</span>,<br>    )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_attention_noparam</span>(<span class="hljs-params"></span><br><span class="hljs-params">        x_test,</span><br><span class="hljs-params">        x_train,</span><br><span class="hljs-params">        attn,</span><br><span class="hljs-params">        num_points=<span class="hljs-number">3</span>,</span><br><span class="hljs-params">        save_path=<span class="hljs-string">&quot;img/visualize_attention_noparam.png&quot;</span>,</span><br><span class="hljs-params">    </span>):<br>        idxs = torch.linspace(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x_test) - <span class="hljs-number">1</span>, num_points).long()<br>        <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(idxs):<br>            plt.figure()<br>            plt.title(<span class="hljs-string">f&quot;Attention for test x=<span class="hljs-subst">&#123;x_test[idx].item():<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br>            plt.plot(x_train, attn[idx].numpy(), <span class="hljs-string">&quot;o-&quot;</span>)<br>            plt.xlabel(<span class="hljs-string">&quot;x_train&quot;</span>)<br>            plt.ylabel(<span class="hljs-string">&quot;Attention weight&quot;</span>)<br>            plt.savefig(<span class="hljs-string">f&quot;img/visualize_attention_noparam_<span class="hljs-subst">&#123;i&#125;</span>.png&quot;</span>)<br>            plt.close()<br><br>    visualize_attention_noparam(x_test_cpu, x_train_cpu, attn)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">visualize_kernel_shape_noparam</span>(<span class="hljs-params"></span><br><span class="hljs-params">        sigma, save_path=<span class="hljs-string">&quot;img/kernelshape_visualize_noparam.png&quot;</span></span><br><span class="hljs-params">    </span>):<br>        diffs = torch.linspace(-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">100</span>)<br>        attn = torch.softmax(-(diffs / sigma).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>) / <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>        plt.figure()<br>        plt.title(<span class="hljs-string">&quot;Fixed Kernel Shape&quot;</span>)<br>        plt.plot(diffs.numpy(), attn.numpy())<br>        plt.xlabel(<span class="hljs-string">&quot;x - center&quot;</span>)<br>        plt.ylabel(<span class="hljs-string">&quot;Kernel value&quot;</span>)<br>        plt.savefig(save_path)<br>        plt.close()<br><br>    visualize_kernel_shape_noparam(sigma)<br><br>    show_heatmaps(<br>        attn.unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>),<br>        xlabel=<span class="hljs-string">&quot;training inputs&quot;</span>,<br>        ylabel=<span class="hljs-string">&quot;testing inputs&quot;</span>,<br>        titles=<span class="hljs-string">&quot;HeatMaps for the final attention (no param)&quot;</span>,<br>        save_path=<span class="hljs-string">&quot;img/heatmap_noparam.png&quot;</span>,<br>    )<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># Parameters</span><br>    width = <span class="hljs-number">20.0</span><br>    n_train = <span class="hljs-number">6000</span><br>    n_test = <span class="hljs-number">6000</span><br>    epochs = <span class="hljs-number">120001</span><br><br>    <span class="hljs-comment"># Run tests</span><br>    x_train, y_train, x_test, y_truth = generate_datasets(width, n_train, n_test)<br>    <span class="hljs-comment"># Train and evaluate</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;For models with no parameters&quot;</span>)<br>    singleNWKernel(<br>        width=width,<br>        n_train=n_train,<br>        n_test=n_test,<br>        x_train=x_train,<br>        y_train=y_train,<br>        x_test=x_test,<br>        y_truth=y_truth<br>    )<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;For models with parameters&quot;</span>)<br>    train(width, epochs, n_train, n_test, x_train, y_train, x_test, y_truth)<br><br></code></pre></td></tr></table></figure>

<h5 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h5><p>首先来看不带参数的版本：</p>
<p><img src="https://s1.imagehub.cc/images/2025/05/18/c939cccff932fb86f1034f4cae4e4e34.png" srcset="/img/loading.gif" lazyload alt="Heatmap for no parameters"></p>
<p><img src="https://s1.imagehub.cc/images/2025/05/18/9a496bc7b7028ff6af05082908b31268.png" srcset="/img/loading.gif" lazyload alt="Regression results"></p>
<div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/c0a83a7ae08417bf42e725bf0daa2d6d.png" srcset="/img/loading.gif" lazyload alt="image"></div><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/cf612c6508dfdcf6a74a1f947477979c.png" srcset="/img/loading.gif" lazyload alt="image"></div><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/98909a5e16f1d8432194b052dd73d169.png" srcset="/img/loading.gif" lazyload alt="image"></div></div></div>

<p>因为没有参数，所以<strong>图像都非常的平滑</strong>！（这其实也是因为数据点采样的均匀性）但是从回归图像可以看出拟合的效果并不好，尤其在有噪声的情况下。</p>
<p>再来看带参数的版本：</p>
<p>下面三张图片分别是<strong>训练5000,50000,120000</strong>个epoch之后的版本。</p>
<blockquote>
<p>其实这里也还是存在过拟合的问题，函数的参数过小，在后面的epoch发生了梯度消失的现象。</p>
</blockquote>
<div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/814912fdc2afcf74bf9e148a227e384c.png" srcset="/img/loading.gif" lazyload alt="image"></div><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/a11039e659f734333340964db3282bae.png" srcset="/img/loading.gif" lazyload alt="image"></div><div class="group-image-wrap"><img src="https://s1.imagehub.cc/images/2025/05/18/bf74c882fb8f6fcabbfde6e62b8076cd.png" srcset="/img/loading.gif" lazyload alt="image"></div></div></div>

<p>从Kernel Shape也可以看出，参数的引入是的<strong>距离效应</strong>显得更显著，即<strong>距离近的点的权重更大</strong>，而<strong>距离远的点的权重变得更小</strong>。</p>
<p><img src="https://s1.imagehub.cc/images/2025/05/18/46a1bbc2e2616cdf8829f63c83942ff3.png" srcset="/img/loading.gif" lazyload alt="Learned Kernel shape"></p>
<p>如果看热力图可以发现，参数的引进导致<strong>图像变的模糊</strong>，这也是数据采样点的效果。</p>
<p><img src="https://s1.imagehub.cc/images/2025/05/18/09d21300c077719e0d26cf21b7f80ed4.png" srcset="/img/loading.gif" lazyload alt="HeatMap for training"></p>
<h3 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h3><p>上文的公式：</p>
<p>$$\alpha(x_q,x_i) &#x3D; \frac{\exp(-\frac{1}{2}(x_q - x_i)^2)}{\sum_{j &#x3D; 1}^{n}\exp(-\frac{1}{2}(x_q - x_j)^2)} &#x3D; \text{softmax}(-\frac{1}{2}(x_q - x_i)^2)$$</p>
<p>可以作为一个<strong>注意力评分函数</strong>，用来衡量查询值$x_q$和单个键值$x_i$的相关性。</p>
<p>我们可以推广到更加高维的空间中：</p>
<p>考虑查询$\vec{\mathbf{q}} \in \mathbb{R}^q$（在$q$维空间中），我们有$m$个键值对：$(\vec{\mathbf{k_1}}, \vec{\mathbf{v_1}})$, $(\vec{\mathbf{k_2}}, \vec{\mathbf{v_2}})$, … , $(\vec{\mathbf{k_m}}, \vec{\mathbf{v_m}})$, 其中$\vec{\mathbf{k_i}} \in \mathbb{R}^k$, $\vec{\mathbf{v_i}} \in \mathbb{R}^v$。</p>
<p>注意这里的<strong>键</strong>和<strong>值</strong>都被推广位高维空间下的向量，并且向量可以是不同维度的（可以粗糙的理解：<strong>翻译前后文本语义空间并不相同</strong>）</p>
<p>那么我们可以做如下的推广计算每一个查询的<strong>权重</strong>。</p>
<p>$$f(\mathbf{q}, (\mathbf{k_1}, \mathbf{v_1}), \ldots, (\mathbf{k_m}, \mathbf{v_m})) &#x3D; \sum_{i&#x3D;1}^m w_i \mathbf{v}_i \in \mathbb{R}^v$$</p>
<p>$$f(\mathbf{q}, (\mathbf{k_1}, \mathbf{v_1}), \ldots, (\mathbf{k_m}, \mathbf{v_m})) &#x3D; \sum_{i&#x3D;1}^m \alpha(\mathbf{q}, \mathbf{k_i}) \mathbf{v_i} \in \mathbb{R}^v$$</p>
<p>$$\alpha(\mathbf{q}, \mathbf{k_i}) &#x3D; \mathrm{softmax}(a(\mathbf{q}, \mathbf{k_i})) &#x3D; \frac{\exp(a(\mathbf{q}, \mathbf{k_i}))}{\sum_{j&#x3D;1}^m \exp(a(\mathbf{q}, \mathbf{k_j}))} \in \mathbb{R}$$</p>
<blockquote>
<p>注意这里的$w_i$是一个标量，也是$\alpha(\mathbf{q}, \mathbf{k}_i)$的输出。</p>
</blockquote>
<p>显然，在更加general的任务上，$q,k,v$的三个维度指标往往不相同。这就导致了<strong>维度不匹配</strong>的情况，单纯的NWKernel公式需要在维度对齐的情况下进行运算（比较两个向量的余弦相似度），因此<strong>后人工作的重点</strong>就是如何设计更好的 <strong>注意力评分函数</strong> $\alpha(\mathbf{q}, \mathbf{k}_i)$。下文介绍两种经典的算法：<strong>加性注意力</strong>和<strong>缩放点积注意力</strong>，以及<strong>Bahdanau</strong>注意力。</p>
<blockquote>
<p>注意力加性函数本身不需要考虑<strong>归一化</strong>的处理方式，因为后面会套一个softmax层。</p>
</blockquote>
<h4 id="掩蔽softmax操作"><a href="#掩蔽softmax操作" class="headerlink" title="掩蔽softmax操作"></a>掩蔽softmax操作</h4><p>回到机器翻译的问题上，在使用注意力模型的时候，为了提高效率采用批量处理的形式（方便矩阵并行运算），但是可能会存在同一批次文本序列长度不一致的问题，这个时候常用的方法是<strong>短序列使用特殊字符填充</strong>，即做padding；此外在做序列生成时，我们不希望机器依赖未来的词元做“抄袭”。以上两种情况本质上即为<strong>不希望全部的key被纳入模型考虑的范围内</strong>（例如特殊字符padding的部分&amp;未来词元等），换句话说，我们需要设置有效长度，超过有效长度的键值对<strong>采用掩蔽操作</strong>，忽略其对模型的影响。</p>
<p>掩蔽的过程在softmax层进行，注意力评分正常计算，但是在进入softmax归一化的时候，对每一个维度设置需要掩蔽的valid_length，因此需要传递进去一个张量，表示每一个批量的valid length，代码见教材，下面给出一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># test_tensor 的维度是(2,2,4)</span><br><span class="hljs-built_in">print</span>(masked_softmax(test_tensor, torch.tensor([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])))<br></code></pre></td></tr></table></figure>

<blockquote>
<p>这里第一个维度2是batchsize，第2个维度是query的数量，第三个维度是键的数量，这里有点容易搞混。</p>
<p>这里做softmax是对最后一个维度的行向量做的，因此在下面的结果也可以看到每一行的和为1</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs lua">tensor(<span class="hljs-string">[[[ 0.4140, -1.1542, -1.2127,  0.6286],</span><br><span class="hljs-string">         [-0.6033,  0.5189, -1.4756, -0.0650]]</span>,<br><br>        <span class="hljs-string">[[-0.1864,  0.5557,  0.1935, -1.2823],</span><br><span class="hljs-string">         [ 0.1995, -1.6036,  1.3123, -0.0660]]</span>])<br>tensor(<span class="hljs-string">[[[0.8275, 0.1725, 0.0000, 0.0000],</span><br><span class="hljs-string">         [0.2456, 0.7544, 0.0000, 0.0000]]</span>,<br><br>        <span class="hljs-string">[[0.2192, 0.4604, 0.3205, 0.0000],</span><br><span class="hljs-string">         [0.2377, 0.0392, 0.7232, 0.0000]]</span>])<br></code></pre></td></tr></table></figure>

<p>例如这里矩阵的第一个维度为2（批量数），对于第一个批量，掩蔽有效长度为2，那么对这个二维矩阵的第三列即之后的列元素都做掩蔽操作，对于第二个批量掩蔽的有效程度为3，显然第四列被做了掩码处理。</p>
<h4 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h4><p>说到底，设计一个好的注意力函数关键在于设计函数：</p>
<p>$$a(\mathbf{q},\mathbf{k_i}) \in \mathbb{R}, \mathbf{q} \in \mathbb{R}^{q}, \mathbf{k_i} \in \mathbb{R}^{k}$$</p>
<p>即<strong>衡量两个不同维度空间下向量的相似度问题</strong>。</p>
<p>加性注意力的做法是<strong>同样扩展到相同维度$h$的向量空间</strong>,即：</p>
<p>$$a(\mathbf{q},\mathbf{k_i})  &#x3D; \mathbf{\omega_{v}}^{\top} \tanh(W_q \mathbf{q}+W_k\mathbf{k} + \mathbf{b}) \in \mathbb{R}, W_q \in \mathbb{R}^{h \times q}, W_k \in \mathbb{R}^{h \times k}, \mathbf{\omega_{v}}^{\top} \in \mathbb{R}^h $$</p>
<p>其实就是一个多层感知机…包含一个参数为h的隐藏层，独特的点是存在<strong>两个维度迁移的可学习矩阵</strong>，进而实现维度的统一后“加性”拼接。</p>
<div class="note note-primary">
            <p><strong>包含一个隐藏层的感知机的形式化表达</strong>：</p><p>$$ \hat{y} &#x3D; \sigma( \mathbf{W_2} ( \sigma(\mathbf{W_1} \mathbf{x} + \mathbf{b_1}) ) + \mathbf{b_2})$$</p><p>在加性注意力中输入的向量就是$\mathbf{q}$和$\mathbf{k}$，从输入层到隐藏层的激活函数是$\tanh$，从隐藏层到输出层再做一个比较简单的点积处理。</p><p>同时，加性注意力一般会禁用偏置项。</p>
          </div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AdditiveAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):<br>        <span class="hljs-comment"># 相关参数的意义：</span><br>        <span class="hljs-comment"># W_k代表k的可学习矩阵，将key_size映射到num_hiddens的维度</span><br>        <span class="hljs-comment"># W_q代表q（查询）的可学习矩阵，将query_size维度映射到num_hiddens的维度</span><br>        <span class="hljs-comment"># w_v代表最后输出层的参数，将num_hiddens维度映射到单个维度</span><br><br>        <span class="hljs-comment"># *这里调用其他的参数**kwargs传递给父类nn.Module的构造函数</span><br>        <span class="hljs-built_in">super</span>(AdditiveAttention, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.w_v = nn.Linear(num_hiddens, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens</span>):<br>        <span class="hljs-comment"># 考虑批量处理，queries，keys和values都是3维张量</span><br>        <span class="hljs-comment"># queries: (batch_size, query_num, query_size), 其中query_size是向量的维度</span><br>        <span class="hljs-comment"># keys: (batch_size, 键值对的个数, key_size)</span><br>        <span class="hljs-comment"># 为了方便后续的维度对齐，扩展到四个维度（后续代码unsqueeze的操作）</span><br>        <span class="hljs-comment"># !其实这里是先做的映射在做的维度扩展</span><br>        <span class="hljs-comment"># *queries: (batch_size, query_num, 1, query_size)</span><br>        <span class="hljs-comment"># *key: (batch_size, 1, 键值对的个数, key_size)</span><br><br>        <span class="hljs-comment"># ! 这一步对上述的两个四维张量的最后一个维度做映射操作，改变最后一个维度</span><br>        queries, keys = <span class="hljs-variable language_">self</span>.W_q(queries), <span class="hljs-variable language_">self</span>.W_k(keys)<br>        features = queries.unsqueeze(<span class="hljs-number">2</span>) + keys.unsqueeze(<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 在维度扩展后，</span><br>        <span class="hljs-comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span><br>        <span class="hljs-comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span><br><br>        <span class="hljs-comment"># 使用广播方式进行求和</span><br>        <span class="hljs-comment"># 广播规则如下：</span><br>        <span class="hljs-comment">#    从最后一个维度开始向前比较</span><br>        <span class="hljs-comment">#    两个维度要么相等，要么其中一个为1，要么其中一个不存在</span><br>        <span class="hljs-comment"># 很明显，在维度扩展之后，广播机制允许我们实现这两个四维张量的加和</span><br>        features = torch.tanh(features)<br>        <span class="hljs-comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span><br><br>        <span class="hljs-comment"># 最后输出的features的形状：(batch_size, 查询的个数，键值对的个数，num_hiddens)，在广播操作之后</span><br>        <span class="hljs-comment"># tanh 作为激活函数会逐元素运算，不会对Tensor的维度产生影响</span><br><br>        <span class="hljs-comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数) (经过点积之后少了一个维度，直接squeeze掉)</span><br>        <span class="hljs-comment"># 将scores传进掩码softmax中，得到Attention weights，形状依然是(batch_size，查询的个数，“键-值”对的个数)</span><br>        scores = <span class="hljs-variable language_">self</span>.w_v(features).squeeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)<br><br>        <span class="hljs-comment"># 为了防止过拟合，对注意力权重矩阵做一定的dropout</span><br>        <span class="hljs-comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br>        <span class="hljs-comment"># 做一次矩阵批量乘法，最后的形状：(batch_size, 查询的个数，值的维度)，相当于对每一个批量的每一个查询都得到了一个属于value空间下的向量</span><br>        <span class="hljs-keyword">return</span> torch.bmm(<span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.attention_weights), values)<br></code></pre></td></tr></table></figure>

<h4 id="缩放点积注意力-Scaled-Dot-Product-Attention"><a href="#缩放点积注意力-Scaled-Dot-Product-Attention" class="headerlink" title="缩放点积注意力 Scaled Dot-Product Attention"></a>缩放点积注意力 Scaled Dot-Product Attention</h4><p>考虑简单的操作，如果$q &#x3D; k$，即$\mathbf{q}$和$\mathbf{k}$位于相同的空间维度下，那么使用余弦相似度计算是最高效也最直接的方法：</p>
<p>$$a(\mathbf{q}, \mathbf{k_i}) &#x3D; \frac{\mathbf{q} \cdot \mathbf{k_i}}{|\mathbf{q}| \times |\mathbf{k_i}|}$$</p>
<p>我们做出简单假设，假设$\mathbf{q}$和$\mathbf{k}$都是均值为0，方差为1的相同维度的向量，那么点积后的均值为0，方差为d，因此在除以一个标量参数$\sqrt{d}$即可得到一个均值为0，方差为1的点积注意力：</p>
<p>$$a(\mathbf q, \mathbf k) &#x3D; \mathbf{q}^\top \mathbf{k}  &#x2F;\sqrt{d}$$</p>
<div class="note note-primary">
            <p><strong>注意，简单的缩放点积注意力只可以用在q和k的维度空间相同的情况下</strong>。</p>
          </div>

<p>同样的，考虑批量处理，得到矩阵乘法的运算结果：</p>
<p>例如基于$n$个查询和$m$个键－值对计算注意力，其中查询和键的长度为$d$，值的长度为$v$。查询$\mathbf Q\in\mathbb R^{n\times d}$、键$\mathbf K\in\mathbb R^{m\times d}$和值$\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是：</p>
<p>$$ \mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DotProductAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(DotProductAttention, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-comment"># queries的形状：(batch_size，查询的个数，d)</span><br>    <span class="hljs-comment"># keys的形状：(batch_size，“键－值”对的个数，d)</span><br>    <span class="hljs-comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span><br>    <span class="hljs-comment"># valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens=<span class="hljs-literal">None</span></span>):<br>        d = queries.shape[-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># 设置transpose_b=True为了交换keys的最后两个维度(不考虑批量，对最后两个维度做转置操作)</span><br>        <span class="hljs-comment"># score的维度：(batch_size, 查询的个数，键值对的个数)</span><br>        <span class="hljs-comment"># 后续的操作和加性注意力一样</span><br>        scores = torch.bmm(queries, keys.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)) / math.sqrt(d)<br>        <span class="hljs-variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)<br>        <span class="hljs-keyword">return</span> torch.bmm(<span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.attention_weights), values)<br></code></pre></td></tr></table></figure>

<h4 id="Bahdanau注意力"><a href="#Bahdanau注意力" class="headerlink" title="Bahdanau注意力"></a>Bahdanau注意力</h4><p>我们已经实现了两种比较简单的Attention Scoring Function，我们尝试对机器翻译任务构建模型：</p>
<p>我们基本的网络结构使用<strong>循环神经网络</strong>，具体而言是基于两个循环神经网络而设计的编码器-解码器架构</p>
<p>这一部分稍后更新~</p>
<h3 id="Multihead-Attention"><a href="#Multihead-Attention" class="headerlink" title="Multihead Attention"></a>Multihead Attention</h3><p>在单头注意力模型中，我们粗糙的通过<strong>相似度的比较</strong>来找到尽可能和查询向量匹配的键值对，而在其中的关键步骤就是<strong>维度对齐</strong>，即施加一个线性变换矩阵获得注意力汇聚输出。</p>
<p>在单头注意力的模式下，我们往往只会使用一个线性变换矩阵，而施加不同的线性变换矩阵产生的效果不相同，因此，如果将单头注意力模型<strong>扩展到多头注意力模型上去</strong>，并施加不同的线性变换矩阵，模型就可以学习到不同的线性变化特征，增加模型的鲁棒性。</p>
<div class="note note-info">
            <p>举一个例子，一个HR在选拔人才是关注人才的表达能力，那么他训练出来的可学习矩阵就会映射到“表达能力”的维度空间上，而不同的HR所关注的点可能并不相同，带来的注意力汇聚也会不相同。</p><p>下面的图片节选自教材。</p>
          </div>

<p><img src="https://zh.d2l.ai/_images/multi-head-attention.svg" srcset="/img/loading.gif" lazyload alt="MultiHead Attention"></p>
<h4 id="形式化表达"><a href="#形式化表达" class="headerlink" title="形式化表达"></a>形式化表达</h4><p>给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，（<strong>三个向量属于不同的维度空间</strong>），考虑有h个注意力头。</p>
<p>每个注意力头$\mathbf{h}_i$（$i &#x3D; 1, \ldots, h$）的计算方法为：</p>
<p>$$\mathbf{h}_i &#x3D; f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},$$</p>
<p>其中，可学习的参数包括</p>
<p>$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$和$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$，这三个可学习矩阵分别把源向量映射到不同的向量空间上。（这是<strong>相对于单头注意力多的部分</strong>，通过施加不同的线性变化进而考虑原向量的不同特征）</p>
<p>对于第i个注意力头，经过线性变换后的三个向量为：$q’_i \in \mathbb{R}^{p_q}, k’_i \in \mathbb{R}^{p_k}, v’_i \in \mathbb{R}^{p_v}$。经过线性变换后的三个向量进入注意力评分函数中，可以是缩放点积注意力也可以是加性注意力。最终输出的维度为：</p>
<p>$$\mathbf{h_i} &#x3D; f(q’_i , k’_i , v’_i ) \in \mathbb{R}^{p_v}$$</p>
<p>这是多头注意力中<strong>多头的部分</strong>，最终经过h个Attention模块后得到一个二维矩阵，也可以写成列向量的形式：</p>
<p>$$\begin{bmatrix}\mathbf h_1\\vdots\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{h \times p_v}$$</p>
<p>最终施加一个全连接层，设计一个可学习矩阵$\mathbf{W_o} \in \mathbb{R}^{p_o \times h \times p_v}$。</p>
<p>$$\mathbf W_o \begin{bmatrix}\mathbf h_1\\vdots\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}$$</p>
<p>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。并且在多头注意力的情况下，单个查询，单个键值对所返回的评分不再是一个标量评分而是一个向量（<strong>因为多头注意力引入了dimension上的注意力</strong>）</p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Artificial-Intelligence/" class="category-chain-item">Artificial Intelligence</a>
  
  
    <span>></span>
    
  <a href="/categories/Artificial-Intelligence/AINN/" class="category-chain-item">AINN</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Artificial-Intelligence/" class="print-no-link">#Artificial Intelligence</a>
      
        <a href="/tags/Deep-Learning/" class="print-no-link">#Deep Learning</a>
      
        <a href="/tags/Neural-Networks/" class="print-no-link">#Neural Networks</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>AINN Attention</div>
      <div>https://xiyuanyang-code.github.io/posts/AINN-Attention/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Xiyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>April 29, 2025</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Updated on</div>
          <div>June 24, 2025</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/AINN-Transformer/" title="AINN Transformer">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">AINN Transformer</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/Docker-Tutorial/" title="Docker Tutorial">
                        <span class="hidden-mobile">Docker Tutorial</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"xiyuanyang-code/xiyuanyang-code.github.io","repo-id":"R_kgDONRhvHQ","category":"Announcements","category-id":"DIC_kwDONRhvHc4ClBnp","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://github.com/xiyuanyang-code" target="_blank" rel="nofollow noopener"><span>YXY</span></a> <a href="https://xiyuanyang-code.github.io/Loving-Count/" target="_blank" rel="nofollow noopener"><span>❤️</span></a> <a href="https://github.com/Siyan-Li" target="_blank" rel="nofollow noopener"><span>LSY</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
