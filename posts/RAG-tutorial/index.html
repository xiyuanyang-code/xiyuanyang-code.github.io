

<!DOCTYPE html>
<html lang="en" >



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/YXY.png">
  <link rel="icon" href="/img/YXY.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#c30000">
  <meta name="author" content="Xiyuan Yang">
  <meta name="keywords" content="Code">
  
    <meta name="description" content="Theoretical explanation and implementation of RAG for LLM uasge.">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG-Tutorial">
<meta property="og:url" content="https://xiyuanyang-code.github.io/posts/RAG-tutorial/index.html">
<meta property="og:site_name" content="Xiyuan Yang&#39;s Blog">
<meta property="og:description" content="Theoretical explanation and implementation of RAG for LLM uasge.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xiyuanyang-code.github.io/img/cover/RAG.jpg">
<meta property="article:published_time" content="2025-03-29T05:29:15.000Z">
<meta property="article:modified_time" content="2025-04-04T03:13:03.921Z">
<meta property="article:author" content="Xiyuan Yang">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="LLMs">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://xiyuanyang-code.github.io/img/cover/RAG.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>RAG-Tutorial - Xiyuan Yang&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xiyuanyang-code.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"❡"},"progressbar":{"enable":true,"height_px":3,"color":"red","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"L7r0uGb0fafbzNvmBADCMH42-gzGzoHsz","app_key":"2Lr1fQ2rjhwRiUrDx0VOQyUm","server_url":null,"path":"window.location.pathname","ignore_local":true},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Xiyuan Yang&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/above/" target="_self">
                <i class="iconfont icon-bookmark-fill"></i>
                <span>Intro</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archive</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Category</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tag</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://xiyuanyang-code.github.io/My-Resume/" target="_self">
                <i class="iconfont icon-code"></i>
                <span>Resume</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>Else</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/FAQ/" target="_self">
                    <i class="iconfont icon-bug"></i>
                    <span>FAQ</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/links" target="_self">
                    <i class="iconfont icon-link-fill"></i>
                    <span>Links</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/All-posts/" target="_self">
                    <i class="iconfont icon-notebook"></i>
                    <span>All-posts</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/Recordings/" target="_self">
                    <i class="iconfont icon-clipcheck"></i>
                    <span>recordings</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/resume/" target="_self">
                    <i class="iconfont icon-github-fill"></i>
                    <span>Github Introduction</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://github.com/xiyuanyang-code" target="_self">
                    <i class="iconfont icon-github-fill"></i>
                    <span>My Github</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/" target="_self">
                    <i class="iconfont icon-copyright"></i>
                    <span>About Hexo</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xiyuanyang-code.github.io/posts/Life-musings/" target="_self">
                    <i class="iconfont icon-brush"></i>
                    <span>Life Musing</span>
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/place.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="RAG-Tutorial"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Xiyuan Yang
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-29 13:29" pubdate>
          March 29, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          778 words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Artificial Intelligence"
        id="heading-5cd2adc9e2a5254e4c1da803519f298b" role="tab" data-toggle="collapse" href="#collapse-5cd2adc9e2a5254e4c1da803519f298b"
        aria-expanded="true"
      >
        Artificial Intelligence
        <span class="list-group-count">(9)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-5cd2adc9e2a5254e4c1da803519f298b"
           role="tabpanel" aria-labelledby="heading-5cd2adc9e2a5254e4c1da803519f298b">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/AI-indepth-reading-AlexNet/" title="AI-Indepth-Reading-AlexNet"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AI-Indepth-Reading-AlexNet</span>
        </a>
      
    
      
      
        <a href="/posts/CS294-1-LLM-Reasoning/" title="CS294-1-LLM-Reasoning"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-1-LLM-Reasoning</span>
        </a>
      
    
      
      
        <a href="/posts/CS294-3-Autogen/" title="CS294-3-Autogen"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">CS294-3-Autogen</span>
        </a>
      
    
      
      
        <a href="/posts/Pre-training-Is-Dead/" title="Pre-Training-Is-Dead?"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Pre-Training-Is-Dead?</span>
        </a>
      
    
      
      
        <a href="/posts/Deep-Learning-Memo/" title="Deep_Learning_Memo"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Deep_Learning_Memo</span>
        </a>
      
    
      
      
        <a href="/posts/RL-speeches/" title="RL_speeches"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">RL_speeches</span>
        </a>
      
    
      
      
        <a href="/posts/AI-Paper-2024/" title="AI-Paper-2024"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AI-Paper-2024</span>
        </a>
      
    
      
      
        <a href="/posts/RAG-tutorial/" title="RAG-Tutorial"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">RAG-Tutorial</span>
        </a>
      
    
      
      
        <a href="/posts/AIBasis-Neural-Networks/" title="AIBasis_Neural_Networks"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">AIBasis_Neural_Networks</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">RAG-Tutorial</h1>
            
            
              <div class="markdown-body">
                
                <style>
  html, body, .markdown-body {
    font-family: Georgia, sans, serif;
  }
</style>

<h1 id="Learn-RAG-from-scratch"><a href="#Learn-RAG-from-scratch" class="headerlink" title="Learn RAG from scratch!"></a>Learn RAG from scratch!</h1><p>Lecture Notes for This Youtube Class.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://www.youtube.com/watch?v=sVcwVQRHIc8">[1]</span></a></sup></p>
<h2 id="Basic-Knowledge"><a href="#Basic-Knowledge" class="headerlink" title="Basic Knowledge"></a>Basic Knowledge</h2><p>Retrieval-Augmented Generation (RAG) is a hybrid framework that combines the strengths of retrieval-based and generative models for natural language processing tasks. The basic principle of RAG involves two key components: <strong>a retriever and a generator</strong>. </p>
<p>The retriever is responsible for fetching relevant documents or information from a large corpus or knowledge base, typically using dense vector representations to find the most pertinent data for a given query. This ensures that the model has access to accurate, up-to-date external information beyond its training data.</p>
<p>The generator, usually a pre-trained language model, then uses this retrieved information to produce coherent and contextually appropriate responses. By grounding its outputs in retrieved facts, RAG reduces hallucinations and improves factual accuracy compared to purely generative models.</p>
<p>Basic Steps:</p>
<ul>
<li><p><strong>Load documents using <code>bs4</code> using Crawler</strong>.</p>
</li>
<li><p>Split text using function of <code>RecursiveCharacterTextSplitter()</code> from <code>langchain.text_splitter</code>.</p>
</li>
<li><p><strong>Embedding splits into vectors</strong>, using <code>Chroma</code> from <code>langchain_community.vectorstores</code> and <code>OpenAIEmbeddings</code> and return the vector store retriever.</p>
</li>
<li><p>Define RAG-chain (including <strong>prompt</strong> and <strong>LLM</strong>)</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Chain</span><br>rag_chain = (<br>    &#123;<span class="hljs-string">&quot;context&quot;</span>: retriever | format_docs, <span class="hljs-string">&quot;question&quot;</span>: RunnablePassthrough()&#125;<br>    | prompt<br>    | llm<br>    | StrOutputParser()<br>)<br></code></pre></td></tr></table></figure>
</li>
<li><p>Using the <code>invoke</code> function to get the generation text!</p>
</li>
</ul>
<h2 id="Query-Translation"><a href="#Query-Translation" class="headerlink" title="Query-Translation"></a>Query-Translation</h2><h3 id="Multi-Query"><a href="#Multi-Query" class="headerlink" title="Multi Query"></a>Multi Query</h3><p><strong>Intuition</strong>: For a large problem, it is likely to encounter situations where “words fail to convey meaning,” and if the answer to the problem is relatively complex, a simple RAG system may not achieve good results through direct vector comparison. Therefore, we can <strong>decompose the problem and use parallel retrieval</strong>.</p>
<p>How to make the splitting works? You can use <strong>Prompt Engineering</strong> and add templates:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><br><span class="hljs-comment"># Multi Query: Different Perspectives</span><br>template = <span class="hljs-string">&quot;&quot;&quot;You are an AI language model assistant. Your task is to generate five </span><br><span class="hljs-string">different versions of the given user question to retrieve relevant documents from a vector </span><br><span class="hljs-string">database. By generating multiple perspectives on the user question, your goal is to help</span><br><span class="hljs-string">the user overcome some of the limitations of the distance-based similarity search. </span><br><span class="hljs-string">Provide these alternative questions separated by newlines. Original question: &#123;question&#125;&quot;&quot;&quot;</span><br>prompt_perspectives = ChatPromptTemplate.from_template(template)<br><br><span class="hljs-keyword">from</span> langchain_core.output_parsers <span class="hljs-keyword">import</span> StrOutputParser<br><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI<br><br>generate_queries = (<br>    prompt_perspectives <br>    | ChatOpenAI(temperature=<span class="hljs-number">0</span>,<br>                 api_key= openai.api_key,<br>                 base_url= BaseUrl) <br>    | StrOutputParser() <br>    | (<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">&quot;\n&quot;</span>))<br>)<br></code></pre></td></tr></table></figure>

<p>In this code, we define the workchain of <strong><code>generate_queries</code></strong>, where we let OpenAI model to split question using prompts.</p>
<p>This is a demo of how AI generate:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs txt">1. How do LLM agents utilize task decomposition in their operations?<br>2. Can you explain the concept of task decomposition as applied to LLM agents?<br>3. What role does task decomposition play in the functioning of LLM agents?<br>4. How is task decomposition integrated into the workflow of LLM agents?<br>5. In what way does task decomposition enhance the capabilities of LLM agents?<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.load <span class="hljs-keyword">import</span> dumps, loads<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_unique_union</span>(<span class="hljs-params">documents: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">list</span>]</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Unique union of retrieved docs &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Flatten list of lists, and convert each Document to string</span><br>    flattened_docs = [dumps(doc) <span class="hljs-keyword">for</span> sublist <span class="hljs-keyword">in</span> documents <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> sublist]<br>    <span class="hljs-comment"># Get unique documents</span><br>    unique_docs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(flattened_docs))<br>    <span class="hljs-comment"># Return</span><br>    <span class="hljs-keyword">return</span> [loads(doc) <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> unique_docs]<br><br><span class="hljs-comment"># Retrieve</span><br>question = <span class="hljs-string">&quot;What is task decomposition for LLM agents?&quot;</span><br>retrieval_chain = generate_queries | retriever.<span class="hljs-built_in">map</span>() | get_unique_union<br>docs = retrieval_chain.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>:question&#125;)<br><span class="hljs-built_in">len</span>(docs)<br><br><span class="hljs-built_in">print</span>(docs)<br></code></pre></td></tr></table></figure>

<p>The core chain of retrieval is <strong><code>retrieval_chain = generate_queries | retriever.map() | get_unique_union</code></strong>, where the input questions is first splitted into subquestions using <code>genegrate_queries</code> and through a <strong>mapping</strong> from the queries to the retriever (for every subquestion, search for the ans_vectors). Finally, using <code>get_unique_union</code> to get the unique answer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain_core.runnables <span class="hljs-keyword">import</span> RunnablePassthrough<br><br><span class="hljs-comment"># RAG</span><br>template = <span class="hljs-string">&quot;&quot;&quot;Answer the following question based on this context:</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">Question: &#123;question&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>prompt = ChatPromptTemplate.from_template(template)<br><br>llm = ChatOpenAI(temperature=<span class="hljs-number">0</span>,<br>                 api_key= openai.api_key,<br>                 base_url= BaseUrl)<br><br>final_rag_chain = (<br>    &#123;<span class="hljs-string">&quot;context&quot;</span>: retrieval_chain, <span class="hljs-string">&quot;question&quot;</span>: itemgetter(<span class="hljs-string">&quot;question&quot;</span>)&#125; <br>    | prompt<br>    | llm<br>    | StrOutputParser()<br>)<br><br>final_rag_chain.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>:question&#125;)<br></code></pre></td></tr></table></figure>

<p>Finally, it is the <strong>RAG</strong> time! The <code>final_rag-chain</code> looks as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">final_rag_chain = (<br>    &#123;<span class="hljs-string">&quot;context&quot;</span>: retrieval_chain, <span class="hljs-string">&quot;question&quot;</span>: itemgetter(<span class="hljs-string">&quot;question&quot;</span>)&#125; <br>    | prompt<br>    | llm<br>    | StrOutputParser()<br>)<br></code></pre></td></tr></table></figure>

<p>We first use <code>retrieval_chain</code> defined above to get the related texts in the documents, add use the <code>itemgetter</code> to get the original problem. Then we feed these into a LLM with the predefined prompt. Finally, we use <code>StrOutputParser()</code> to get the final answer of LLM.</p>
<h3 id="RAG-Fusions"><a href="#RAG-Fusions" class="headerlink" title="RAG Fusions"></a>RAG Fusions</h3><p><strong>Intuition</strong>: In Multi Query, RAG performs a crude deduplication and merging operation on the documents obtained for each subproblem, which is unreasonable because the weights corresponding to each problem are different, and the weights (similarities) of the answers obtained for each subproblem are also different. Therefore, the core idea of RAG fusion is to <strong>use a multi-retrieval fusion method based on Reciprocal Rank Fusion (RRF)</strong>, calculating the corresponding weights for each document before outputting them to obtain more reasonable results.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.load <span class="hljs-keyword">import</span> dumps, loads<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reciprocal_rank_fusion</span>(<span class="hljs-params">results: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">list</span>], k=<span class="hljs-number">60</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Reciprocal_rank_fusion that takes multiple lists of ranked documents </span><br><span class="hljs-string">        and an optional parameter k used in the RRF formula &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># Initialize a dictionary to hold fused scores for each unique document</span><br>    fused_scores = &#123;&#125;<br><br>    <span class="hljs-comment"># Iterate through each list of ranked documents</span><br>    <span class="hljs-keyword">for</span> docs <span class="hljs-keyword">in</span> results:<br>        <span class="hljs-comment"># Iterate through each document in the list, with its rank (position in the list)</span><br>        <span class="hljs-keyword">for</span> rank, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(docs):<br>            <span class="hljs-comment"># Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)</span><br>            doc_str = dumps(doc)<br>            <span class="hljs-comment"># If the document is not yet in the fused_scores dictionary, add it with an initial score of 0</span><br>            <span class="hljs-keyword">if</span> doc_str <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> fused_scores:<br>                fused_scores[doc_str] = <span class="hljs-number">0</span><br>            <span class="hljs-comment"># Retrieve the current score of the document, if any</span><br>            previous_score = fused_scores[doc_str]<br>            <span class="hljs-comment"># Update the score of the document using the RRF formula: 1 / (rank + k)</span><br>            fused_scores[doc_str] += <span class="hljs-number">1</span> / (rank + k)<br><br>    <span class="hljs-comment"># Sort the documents based on their fused scores in descending order to get the final reranked results</span><br>    reranked_results = [<br>        (loads(doc), score)<br>        <span class="hljs-keyword">for</span> doc, score <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(fused_scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br>    ]<br><br>    <span class="hljs-comment"># Return the reranked results as a list of tuples, each containing the document and its fused score</span><br>    <span class="hljs-keyword">return</span> reranked_results<br><br>retrieval_chain_rag_fusion = generate_queries | retriever.<span class="hljs-built_in">map</span>() | reciprocal_rank_fusion<br>docs = retrieval_chain_rag_fusion.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>: question&#125;)<br><span class="hljs-built_in">len</span>(docs)<br></code></pre></td></tr></table></figure>

<h3 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h3><p>Decomposition essentially uses a simpler concatenation method to hierarchically link problems together, similar to the step-by-step reasoning in Chain of Thought (CoT). It guides the model to decompose problems through prompts and, based on the solutions to previous problems, adds new retrievals on top of the previous problem’s retrievals.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><br><span class="hljs-comment"># Decomposition</span><br>template = <span class="hljs-string">&quot;&quot;&quot;You are a helpful assistant that generates multiple sub-questions related to an input question. \n</span><br><span class="hljs-string">The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n</span><br><span class="hljs-string">Generate multiple search queries related to: &#123;question&#125; \n</span><br><span class="hljs-string">Output (3 queries):&quot;&quot;&quot;</span><br>prompt_decomposition = ChatPromptTemplate.from_template(template)<br></code></pre></td></tr></table></figure>

<p>Answer recursively</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Prompt</span><br>template = <span class="hljs-string">&quot;&quot;&quot;Here is the question you need to answer:</span><br><span class="hljs-string"></span><br><span class="hljs-string">\n --- \n &#123;question&#125; \n --- \n</span><br><span class="hljs-string"></span><br><span class="hljs-string">Here is any available background question + answer pairs:</span><br><span class="hljs-string"></span><br><span class="hljs-string">\n --- \n &#123;q_a_pairs&#125; \n --- \n</span><br><span class="hljs-string"></span><br><span class="hljs-string">Here is additional context relevant to the question: </span><br><span class="hljs-string"></span><br><span class="hljs-string">\n --- \n &#123;context&#125; \n --- \n</span><br><span class="hljs-string"></span><br><span class="hljs-string">Use the above context and any background question + answer pairs to answer the question: \n &#123;question&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>decomposition_prompt = ChatPromptTemplate.from_template(template)<br></code></pre></td></tr></table></figure>

<blockquote>
<p><strong>Parallel structures are suitable for parallel problems, while hierarchical structures are appropriate for reasoning problems</strong>. It is necessary to choose the appropriate RAG architecture based on different inquiries.</p>
</blockquote>
<h3 id="Step-Back"><a href="#Step-Back" class="headerlink" title="Step Back"></a>Step Back</h3><p>Step Back Prompting<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/pdf/2310.06117">[2]</span></a></sup> is a fancy way proposed by Google DeepMind, which uses Prompt Engineering to make the Queries more abstract in order to decrease noise.</p>
<p><img src="https://s1.imagehub.cc/images/2025/04/04/b2538ca755acad7e0bae5e08cbd91fb0.png" srcset="/img/loading.gif" lazyload alt="Step Back Prompting"> </p>
<p>It is quite similar to <strong>Chain of Thought</strong>, but it will focus more on the <strong>abstraction</strong> process, which will distill key questions from specific contexts to achieve better results in the RAG process.</p>
<p>Let’s just see the prompt!</p>
<p>Moreover, it uses <strong>few-shot learning</strong> for learning template, which performs better than <strong>zero-shot learning</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Few Shot Examples</span><br><span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate, FewShotChatMessagePromptTemplate<br>examples = [<br>    &#123;<br>        <span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;Could the members of The Police perform lawful arrests?&quot;</span>,<br>        <span class="hljs-string">&quot;output&quot;</span>: <span class="hljs-string">&quot;what can the members of The Police do?&quot;</span>,<br>    &#125;,<br>    &#123;<br>        <span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;Jan Sindel’s was born in what country?&quot;</span>,<br>        <span class="hljs-string">&quot;output&quot;</span>: <span class="hljs-string">&quot;what is Jan Sindel’s personal history?&quot;</span>,<br>    &#125;,<br>]<br><span class="hljs-comment"># We now transform these to example messages</span><br>example_prompt = ChatPromptTemplate.from_messages(<br>    [<br>        (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;&#123;input&#125;&quot;</span>),<br>        (<span class="hljs-string">&quot;ai&quot;</span>, <span class="hljs-string">&quot;&#123;output&#125;&quot;</span>),<br>    ]<br>)<br>few_shot_prompt = FewShotChatMessagePromptTemplate(<br>    example_prompt=example_prompt,<br>    examples=examples,<br>)<br>prompt = ChatPromptTemplate.from_messages(<br>    [<br>        (<br>            <span class="hljs-string">&quot;system&quot;</span>,<br>            <span class="hljs-string">&quot;&quot;&quot;You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:&quot;&quot;&quot;</span>,<br>        ),<br>        <span class="hljs-comment"># Few shot examples</span><br>        few_shot_prompt,<br>        <span class="hljs-comment"># New question</span><br>        (<span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;&#123;question&#125;&quot;</span>),<br>    ]<br>)<br></code></pre></td></tr></table></figure>

<p>And for the response prompt:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Response prompt </span><br>response_prompt_template = <span class="hljs-string">&quot;&quot;&quot;You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.</span><br><span class="hljs-string"></span><br><span class="hljs-string"># &#123;normal_context&#125;</span><br><span class="hljs-string"># &#123;step_back_context&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string"># Original Question: &#123;question&#125;</span><br><span class="hljs-string"># Answer:&quot;&quot;&quot;</span><br>response_prompt = ChatPromptTemplate.from_template(response_prompt_template)<br><br>chain = (<br>    &#123;<br>        <span class="hljs-comment"># Retrieve context using the normal question</span><br>        <span class="hljs-string">&quot;normal_context&quot;</span>: RunnableLambda(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;question&quot;</span>]) | retriever,<br>        <span class="hljs-comment"># Retrieve context using the step-back question</span><br>        <span class="hljs-string">&quot;step_back_context&quot;</span>: generate_queries_step_back | retriever,<br>        <span class="hljs-comment"># Pass on the question</span><br>        <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;question&quot;</span>],<br>    &#125;<br>    | response_prompt<br>    | ChatOpenAI(temperature=<span class="hljs-number">0</span>)<br>    | StrOutputParser()<br>)<br><br>chain.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>: question&#125;)<br></code></pre></td></tr></table></figure>

<p>It feed AI with both the answers <strong>with normal questions and with step_back contents</strong>.</p>
<h3 id="HyDE"><a href="#HyDE" class="headerlink" title="HyDE"></a>HyDE</h3><p>In practical situations, questions and texts exist in two completely different spaces. Directly embedding them can lead to errors and noise interference. Therefore, we need to <strong>ensure consistency between the two spaces</strong>.</p>
<div class="note note-primary">
            <p>The difficulty of zero-shot dense retrieval lies precisely: it requires learning of two embedding functions (for query and document respectively) <strong>into the same embedding space</strong> where inner product captures relevance. Without relevance judgments&#x2F;scores to fit, learning becomes intractable.</p>
          </div>

<p>Let’s just see the abstract part for the passage:</p>
<div class="note note-primary">
            <p>Given a query, HyDE first <strong>zero-shot instructs an instruction-following language model</strong> (e.g. <code>InstructGPT</code>) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, <strong>an unsupervised contrastively learned encoder</strong><del>(e.g. <code>Contriever</code>) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, <strong>where similar real documents are retrieved based on vector similarity</strong>. This second step ground the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages</del>(e.g. sw, ko, ja).</p>
          </div>

<p>That is what <strong>HyDE</strong><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://arxiv.org/abs/2212.10496">[3]</span></a></sup> for! The core idea of HyDE is to generate a hypothetical document that provides richer contextual information for the retrieval process. In HyDE, we need to generate a new hypothetical passage to simulate all possible answers.</p>
<p>Firstly, we need to generate a new hypothetical passage:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><br><span class="hljs-comment"># HyDE document genration</span><br>template = <span class="hljs-string">&quot;&quot;&quot;Please write a scientific paper passage to answer the question</span><br><span class="hljs-string">Question: &#123;question&#125;</span><br><span class="hljs-string">Passage:&quot;&quot;&quot;</span><br>prompt_hyde = ChatPromptTemplate.from_template(template)<br><br><span class="hljs-keyword">from</span> langchain_core.output_parsers <span class="hljs-keyword">import</span> StrOutputParser<br><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> ChatOpenAI<br><br>generate_docs_for_retrieval = (<br>    prompt_hyde | ChatOpenAI(temperature=<span class="hljs-number">0</span>) | StrOutputParser() <br>)<br><br><span class="hljs-comment"># Run</span><br>question = <span class="hljs-string">&quot;What is task decomposition for LLM agents?&quot;</span><br>generate_docs_for_retrieval.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>:question&#125;)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Retrieve</span><br>retrieval_chain = generate_docs_for_retrieval | retriever <br>retireved_docs = retrieval_chain.invoke(&#123;<span class="hljs-string">&quot;question&quot;</span>:question&#125;)<br>retireved_docs<br></code></pre></td></tr></table></figure>

<p><code>retrieval_chain = generate_docs_for_retrieval | retriever</code> is the most fundamental part! After generating a hypothetical passage for retrieval, we then feed the answer into the retriever.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># RAG</span><br>template = <span class="hljs-string">&quot;&quot;&quot;Answer the following question based on this context:</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">Question: &#123;question&#125;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>prompt = ChatPromptTemplate.from_template(template)<br><br>final_rag_chain = (<br>    prompt<br>    | llm<br>    | StrOutputParser()<br>)<br><br>final_rag_chain.invoke(&#123;<span class="hljs-string">&quot;context&quot;</span>:retireved_docs,<span class="hljs-string">&quot;question&quot;</span>:question&#125;)<br></code></pre></td></tr></table></figure>

<h2 id="Routing"><a href="#Routing" class="headerlink" title="Routing"></a>Routing</h2><p><strong>Basic Principle</strong>: Routing the decomposed question into the right vector space.</p>
<p>It includes:</p>
<ul>
<li>Logical Routing</li>
<li>Semantic Routing</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=sVcwVQRHIc8">https://www.youtube.com/watch?v=sVcwVQRHIc8</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.06117">https://arxiv.org/pdf/2310.06117</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.10496">https://arxiv.org/abs/2212.10496</a>
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Artificial-Intelligence/" class="category-chain-item">Artificial Intelligence</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Artificial-Intelligence/" class="print-no-link">#Artificial Intelligence</a>
      
        <a href="/tags/Deep-Learning/" class="print-no-link">#Deep Learning</a>
      
        <a href="/tags/LLMs/" class="print-no-link">#LLMs</a>
      
        <a href="/tags/RAG/" class="print-no-link">#RAG</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>RAG-Tutorial</div>
      <div>https://xiyuanyang-code.github.io/posts/RAG-tutorial/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Xiyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 29, 2025</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Updated on</div>
          <div>April 4, 2025</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/RAG-Blog-Content-Retrieval/" title="RAG-Blog-Content-Retrieval">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">RAG-Blog-Content-Retrieval</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/Algorithm-BFS-DFS/" title="Algorithm-BFS-DFS">
                        <span class="hidden-mobile">Algorithm-BFS-DFS</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"xiyuanyang-code/xiyuanyang-code.github.io","repo-id":"R_kgDONRhvHQ","category":"Announcements","category-id":"DIC_kwDONRhvHc4ClBnp","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://github.com/xiyuanyang-code" target="_blank" rel="nofollow noopener"><span>YXY</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/Siyan-Li" target="_blank" rel="nofollow noopener"><span>LSY</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
